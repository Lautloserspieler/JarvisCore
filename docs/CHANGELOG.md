# Changelog

Alle bedeutenden Ã„nderungen an diesem Projekt werden in dieser Datei dokumentiert.

## [1.0.1] - 2025-12-16, 11:10 CET

### âœ¨ Major Features Added

#### ğŸ§  llama.cpp Lokale Inferenz - FERTIGGESTELLT!
- **NEU:** `core/llama_inference.py` - VollstÃ¤ndige llama.cpp Inference Engine
- **NEU:** GPU-Acceleration mit CUDA (n_gpu_layers=-1)
- **NEU:** Chat-Modus mit History-Support
- **NEU:** Text-Generation mit vollstÃ¤ndiger Parameterkontrolle
- **NEU:** Thread-Safe Model Loading/Unloading
- **NEU:** Status-API fÃ¼r Live-Monitoring

**Features:**
- âœ… GGUF Model Loading (Mistral, Qwen, DeepSeek, Llama 2)
- âœ… Context-Management bis 32K Tokens
- âœ… Automatische CUDA-Erkennung
- âœ… Memory-efficient mit Garbage Collection
- âœ… Performance: ~30-50 tok/s (GPU), ~5-10 tok/s (CPU)

**Backend Integration:**
- ğŸ”„ `backend/main.py` - Integration von llama_inference
- ğŸ”„ WebSocket-Chat nutzt jetzt echte AI-Responses
- ğŸ”„ API-Endpunkte fÃ¼r Model-Management

```python
# Verwendung
from core.llama_inference import llama_runtime

llama_runtime.load_model(
    model_path="models/llm/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    model_name="mistral",
    n_ctx=8192
)

result = llama_runtime.chat(
    message="Hallo JARVIS!",
    history=[],
    temperature=0.7
)
print(result['text'])  # Echte AI-Antwort!
```

### ğŸ“ Dokumentation
- âœ… `IMPLEMENTATION_STATUS.md` - llama.cpp als "Production Ready" markiert
- âœ… `docs/ARCHITECTURE.md` - llama.cpp Architektur dokumentiert
- âœ… `README.md` - Features aktualisiert (v1.0.1)
- âœ… `README_GB.md` - Englische Version aktualisiert
- âœ… `docs/CHANGELOG.md` - Dieser Changelog erstellt

### ğŸ› Bugfixes
- âœ… Backend nutzte `hf_inference.py` (nicht existent) - ersetzt durch `llama_inference.py`
- âœ… Mock-Chat-Responses durch echte LLM-Inferenz ersetzt

### âš ï¸ Deprecated
- âŒ `hf_inference.py` wurde nie implementiert (entfernt aus PlÃ¤nen)
- âŒ HuggingFace Transformers Integration verworfen (llama.cpp ist besser)

---

## [1.0.0] - 2025-12-14

### âœ¨ Initial Release

#### ğŸ“¦ LLM Download-System (Ollama-Style)
- `core/model_downloader.py` - Advanced Download-Engine
- `core/model_registry.py` - Multi-Registry Support
- `core/model_manifest.py` - Metadata-Management
- `core/llm_manager.py` - High-Level LLM-Management

**Features:**
- âœ… Multi-Registry (HuggingFace, Ollama, Custom)
- âœ… Resume-Support (HTTP Range Requests)
- âœ… SHA256-Verifizierung
- âœ… Progress-Callbacks (Speed, ETA)
- âœ… Quantization-Varianten

#### ğŸ¨ Models-Page (React Frontend)
- `frontend/src/pages/ModelsPage.tsx` - Model-Grid
- `frontend/src/components/models/ModelCard.tsx` - Model-Karten
- `frontend/src/components/models/DownloadQueue.tsx` - Download-Queue
- `frontend/src/components/models/VariantDialog.tsx` - Varianten-Auswahl

**Features:**
- âœ… Live-Progress-Tracking (SSE)
- âœ… Download-Queue (Sticky Panel)
- âœ… Cancel-Downloads
- âœ… Status-Badges
- âœ… Dark Mode (shadcn/ui)

#### ğŸš€ Backend API
- `backend/main.py` - FastAPI Server
- `main.py` (Root) - Unified Launcher

**Features:**
- âœ… REST-API fÃ¼r Models, Chat, Plugins, Logs
- âœ… WebSocket-Support
- âœ… SSE fÃ¼r Progress-Tracking
- âœ… Auto-Port-Detection

#### ğŸ“š Dokumentation
- `README.md` - VollstÃ¤ndige deutsche Dokumentation
- `README_GB.md` - Englische Version
- `docs/LLM_DOWNLOAD_SYSTEM.md` - Download-System Details
- `docs/ARCHITECTURE.md` - Architektur-Ãœbersicht
- `IMPLEMENTATION_STATUS.md` - Feature-Status

---

## Kategorien-Legende

- **Added** - Neue Features
- **Changed** - Ã„nderungen an bestehenden Features
- **Deprecated** - Bald entfernte Features
- **Removed** - Entfernte Features
- **Fixed** - Bugfixes
- **Security** - Sicherheits-Updates

---

**Versionierungs-Schema:** [Semantic Versioning 2.0.0](https://semver.org/)

- **MAJOR** (1.x.x) - Inkompatible API-Ã„nderungen
- **MINOR** (x.1.x) - Neue Features, abwÃ¤rtskompatibel
- **PATCH** (x.x.1) - Bugfixes, abwÃ¤rtskompatibel
