# ü§ñ JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

Ein moderner KI-Assistent mit holographischer UI und **vollst√§ndig lokaler llama.cpp Inferenz**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-cyan.svg)](https://golang.org)
[![Vue](https://img.shields.io/badge/Vue-3.5+-green.svg)](https://vuejs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

[üá¨üáß English Version](./README_GB.md)

</div>

---

## ‚ú® Features

### üß† KI-Engine
- ‚úÖ **llama.cpp Lokale Inferenz** - Vollst√§ndig implementiert und funktionsf√§hig!
- ‚úÖ **GPU-Acceleration** - Automatische CUDA-Erkennung (30-50 tok/s)
- ‚úÖ **4 GGUF-Modelle** - Mistral, Qwen, DeepSeek, Llama 2 (Q4_K_M)
- ‚úÖ **Chat mit History** - Kontext-bewusste Konversationen
- ‚úÖ **Bis 32K Context** - Lange Konversationen m√∂glich
- ‚úÖ **System-Prompts** - JARVIS-Pers√∂nlichkeit konfigurierbar

### üé® Frontend (Vue 3)
- ‚úÖ **Holographische UI** - Beeindruckende JARVIS-inspirierte Benutzeroberfl√§che
- ‚úÖ **Echtzeit-Chat** - WebSocket-basierte Live-Kommunikation
- ‚úÖ **Sprach-Interface** - Voice-Input mit visueller R√ºckmeldung
- ‚úÖ **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Settings
- ‚úÖ **Model-Management** - Download und Verwaltung von KI-Modellen
- ‚úÖ **Responsive Design** - Funktioniert auf allen Bildschirmgr√∂√üen
- ‚úÖ **Dark Theme** - Cyberpunk-√Ñsthetik mit leuchtenden Effekten

### üöÄ Backend (Python + Go)
- ‚úÖ **FastAPI Server** - Hochperformanter Python Backend
- ‚úÖ **Go Microservices** - Gateway, Memory, Speech Services
- ‚úÖ **llama.cpp Integration** - Native GGUF-Model-Inferenz
- ‚úÖ **WebSocket Support** - Echtzeitkommunikation
- ‚úÖ **RESTful API** - Vollst√§ndige REST-Endpunkte
- ‚úÖ **Plugin System** - Erweiterbare Architektur
- ‚úÖ **Memory Storage** - Konversationshistorie & Kontext

---

## üöÄ Schnellstart

### Voraussetzungen
- **Docker** & **Docker Compose** (empfohlen)
- *ODER* Python 3.11+, Go 1.21+, Node.js 18+
- **(Optional)** NVIDIA GPU mit CUDA f√ºr beschleunigte Inferenz

### üê≥ Installation mit Docker (Empfohlen)

```bash
# Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Alle Services mit Docker Compose starten
docker-compose up -d

# Logs verfolgen
docker-compose logs -f
```

**Das war's!** Docker Compose startet automatisch:
- ‚úÖ Backend (Python/FastAPI)
- ‚úÖ Frontend (Vue 3 + Vite)
- ‚úÖ Go Gateway Service
- ‚úÖ Memory Service
- ‚úÖ Speech Service

### üîß Alternative: Manueller Start (Development)

Wenn du ohne Docker entwickeln m√∂chtest:

```bash
# Mit dem einheitlichen Launcher
python main.py
```

Das `main.py` Script:
1. ‚úÖ Pr√ºft alle Anforderungen
2. ‚úÖ Installiert fehlende Abh√§ngigkeiten
3. ‚úÖ Startet Backend & Frontend parallel
4. ‚úÖ √ñffnet Browser automatisch

**Oder manuell je Service:**

```bash
# Backend
cd backend
pip install -r requirements.txt
python main.py

# Frontend (separates Terminal)
cd frontend
npm install
npm run dev

# Go Services (separates Terminal)
cd go-services/gateway
go run cmd/gateway/main.go
```

---

## üåê Zugriffspunkte

Nach dem Start erreichst du:

- üé® **Frontend UI**: http://localhost:5000
- üîß **Backend API**: http://localhost:5050
- üåê **Go Gateway**: http://localhost:8080
- üìö **API-Dokumentation**: http://localhost:5050/docs
- üîå **WebSocket**: ws://localhost:5050/ws

---

## üß† llama.cpp Lokale Inferenz

**NEU in v1.1.0** - Production-ready mit Docker-Support!

### Features
- üöÄ **GPU-Acceleration** - CUDA automatisch erkannt
- üéØ **GGUF-Support** - Alle llama.cpp-kompatiblen Modelle
- üí¨ **Chat-Modus** - History mit bis zu 32K Context
- ‚ö° **Performance** - 30-50 tokens/sec (GPU), 5-10 tokens/sec (CPU)
- üê≥ **Docker-Ready** - Plug & Play Container-Deployment

### Verf√ºgbare Modelle

| Model | Gr√∂√üe | Use Case | Performance |
|-------|-------|----------|-------------|
| **Mistral 7B Nemo** | ~7.5 GB | Code, technische Details | ‚ö°‚ö°‚ö° |
| **Qwen 2.5 7B** | ~5.2 GB | Vielseitig, multilingual | ‚ö°‚ö°‚ö° |
| **DeepSeek R1 8B** | ~6.9 GB | Analysen, Reasoning | ‚ö°‚ö° |
| **Llama 2 7B** | ~4.0 GB | Kreativ, Chat | ‚ö°‚ö°‚ö° |

### Verwendung

```python
from core.llama_inference import llama_runtime

# Modell laden
llama_runtime.load_model(
    model_path="models/llm/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    model_name="mistral",
    n_ctx=8192,
    n_gpu_layers=-1
)

# Chat mit History
result = llama_runtime.chat(
    message="Erkl√§re mir Quantencomputing",
    history=[...],
    system_prompt="Du bist JARVIS...",
    temperature=0.7
)
```

---

## üì¶ Model-Download-System

JARVIS Core nutzt ein **Ollama-inspiriertes Download-System**:

### Features
- üîÑ **Multi-Registry-Support** - HuggingFace, Ollama, Custom URLs
- üì¶ **Resume-Downloads** - Unterbrochene Downloads fortsetzen
- ‚úÖ **SHA256-Verifizierung** - Automatische Integrit√§tspr√ºfung
- üìä **Live-Progress** - Speed, ETA, Fortschrittsbalken
- üîê **HuggingFace Token** - Support f√ºr private Repos

### Models verwalten

1. **Web-UI √∂ffnen**: http://localhost:5000
2. **Models-Tab**: Navigation zur Model-Verwaltung
3. **Model downloaden**: Klick "Download" ‚Üí W√§hle Quantization
4. **Model laden**: Klick "Load" bei heruntergeladenem Modell
5. **Chat starten**: Gehe zu "Chat" Tab und schreibe

Weitere Infos: [docs/LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## üìÅ Projektstruktur

```
JarvisCore/
‚îú‚îÄ‚îÄ docker-compose.yml      # üê≥ Docker Orchestration
‚îú‚îÄ‚îÄ main.py                 # üöÄ Unified Launcher (dev)
‚îú‚îÄ‚îÄ core/                   # üß† Core Python Modules
‚îÇ   ‚îú‚îÄ‚îÄ llama_inference.py # llama.cpp Engine
‚îÇ   ‚îú‚îÄ‚îÄ model_downloader.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ backend/                # üîß Python/FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ frontend/               # üé® Vue 3 Frontend
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ go-services/            # ‚ö° Go Microservices
‚îÇ   ‚îú‚îÄ‚îÄ gateway/           # API Gateway
‚îÇ   ‚îú‚îÄ‚îÄ memory/            # Memory Service
‚îÇ   ‚îî‚îÄ‚îÄ speech/            # Speech Processing
‚îú‚îÄ‚îÄ models/llm/             # üì¶ GGUF Models
‚îú‚îÄ‚îÄ docs/                   # üìö Documentation
‚îî‚îÄ‚îÄ README.md
```

---

## üê≥ Docker Commands

```bash
# Starten
docker-compose up -d

# Stoppen
docker-compose down

# Logs anzeigen
docker-compose logs -f

# Neu builden
docker-compose build

# Services neustarten
docker-compose restart

# Bestimmten Service neustarten
docker-compose restart backend
```

---

## üîå API-Endpunkte

### Chat
- `WS /ws` - WebSocket-Chat mit AI
- `GET /api/chat/sessions` - Chat-Sessions
- `POST /api/chat/sessions` - Neue Session

### Models
- `GET /api/models` - Alle Modelle
- `POST /api/models/{id}/load` - Modell laden
- `POST /api/models/download` - Download starten
- `DELETE /api/models/delete` - Modell l√∂schen

### System
- `GET /api/health` - Health-Check
- `GET /api/logs` - System-Logs

Vollst√§ndige API-Docs: http://localhost:5050/docs

---

## üé® Technologie-Stack

### Frontend
- **Framework**: Vue 3 + TypeScript
- **Build**: Vite
- **UI**: Tailwind CSS + Custom Components
- **State**: Pinia
- **WebSocket**: Native API

### Backend
- **Python**: FastAPI + Uvicorn
- **Go**: Fiber (Microservices)
- **AI**: llama.cpp + CUDA
- **WebSocket**: FastAPI WebSocket

### Infrastructure
- **Container**: Docker + Docker Compose
- **Reverse Proxy**: Go Gateway
- **Storage**: Local File System

---

## üéØ Roadmap

### ‚úÖ v1.1.0 (Current) - Dezember 2025
- ‚úÖ Docker Compose Setup
- ‚úÖ Go Microservices
- ‚úÖ Vue 3 Migration
- ‚úÖ Production-ready llama.cpp
- ‚úÖ Community Documentation

### üîÑ v1.2.0 - Q1 2026
- Voice Input (Whisper)
- Voice Output (XTTS v2)
- Desktop App (Wails)
- Enhanced Memory System

### üìã v2.0.0 - Q2 2026
- RAG Implementation
- Vector Database
- Multi-User Support
- Cloud Deployment
- Mobile App

---

## ü§ù Contributing

Beitr√§ge sind willkommen! Bitte lies [CONTRIBUTING.md](CONTRIBUTING.md) f√ºr Details.

### Quick Start f√ºr Contributors

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Commit deine √Ñnderungen (`git commit -m 'feat: Add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. Erstelle einen Pull Request

Bitte beachte:
- [Code of Conduct](CODE_OF_CONDUCT.md)
- [Security Policy](SECURITY.md)
- [Coding Standards](CONTRIBUTING.md#coding-standards)

---

## üìÑ Lizenz

**Apache License 2.0** mit zus√§tzlicher kommerzieller Einschr√§nkung.

Dieses Projekt ist unter der Apache License 2.0 lizenziert mit folgender **zus√§tzlicher Einschr√§nkung**:

> **Kommerzielle Nutzung, Verkauf oder Weitervertrieb dieser Software ist ohne vorherige schriftliche Genehmigung des Copyright-Inhabers untersagt.**

Vollst√§ndige Lizenz: [LICENSE](./LICENSE)

---

## üîí Security

Sicherheitsl√ºcken bitte **nicht** als GitHub Issue melden. Nutze stattdessen:
- GitHub Security Advisory
- Email (siehe [SECURITY.md](SECURITY.md))

Weitere Infos: [SECURITY.md](SECURITY.md)

---

## üôè Danksagungen

- Inspiriert von JARVIS aus Iron Man
- Gebaut mit [Vue 3](https://vuejs.org/)
- Backend mit [FastAPI](https://fastapi.tiangolo.com/)
- Lokale Inferenz mit [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Microservices mit [Go Fiber](https://gofiber.io/)
- Containerisierung mit [Docker](https://docker.com)

---

## üìö Weitere Dokumentation

- [Contributing Guidelines](CONTRIBUTING.md)
- [Code of Conduct](CODE_OF_CONDUCT.md)
- [Security Policy](SECURITY.md)
- [LLM Download System](docs/LLM_DOWNLOAD_SYSTEM.md)
- [Architecture](docs/ARCHITECTURE.md)
- [Changelog](docs/CHANGELOG.md)

---

<div align="center">

**Erstellt mit ‚ù§Ô∏è von Lautloserspieler**

*"Manchmal muss man rennen, bevor man gehen kann."* - Tony Stark

**Version:** 1.1.0 | **Release:** 02. Januar 2026

[‚≠ê Star us on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [üêõ Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [üí° Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>
