{
  "run": [
    {
      "method": "shell.run",
      "params": {
        "message": "conda install python=3.11.9 git -y"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "message": "python -m venv venv"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "python -m pip install -U pip setuptools wheel"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "pip install numpy==1.26.4 cython"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "pip install -e \".[tts]\""
      }
    },
    {
      "method": "input",
      "params": {
        "title": "GPU Backend",
        "description": "W√§hle deinen GPU-Typ:",
        "type": "select",
        "options": [
          {
            "title": "üîµ CPU (alle Computer)",
            "value": "cpu"
          },
          {
            "title": "üü¢ NVIDIA CUDA (RTX/GTX)",
            "value": "cuda"
          },
          {
            "title": "üü† AMD ROCm (Linux)",
            "value": "rocm"
          },
          {
            "title": "üçé Apple Metal (macOS)",
            "value": "metal"
          }
        ]
      }
    },
    {
      "method": "local.set",
      "params": {
        "gpu": "{{input}}"
      }
    },
    {
      "when": "{{local.gpu === 'cpu'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu === 'cuda'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "FORCE_CMAKE": "1",
          "CMAKE_ARGS": "-DGGML_CUDA=on"
        },
        "message": "pip uninstall -y llama-cpp-python && pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu === 'rocm'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "FORCE_CMAKE": "1",
          "CMAKE_ARGS": "-DGGML_HIPBLAS=on"
        },
        "message": "pip uninstall -y llama-cpp-python && pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu === 'metal'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "FORCE_CMAKE": "1",
          "CMAKE_ARGS": "-DGGML_METAL=on"
        },
        "message": "pip uninstall -y llama-cpp-python && pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "path": "frontend",
        "message": "npm install"
      }
    },
    {
      "method": "notify",
      "params": {
        "html": "Installation abgeschlossen!<br><br><b>Python:</b> 3.11.9<br><b>GPU:</b> {{input}}<br><br>Klicke 'Start' zum Starten."
      }
    }
  ]
}