# ğŸ¤– JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

[![CI/CD](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/ci.yml/badge.svg)](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/ci.yml)
[![Release](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/release.yml/badge.svg)](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/release.yml)
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-cyan.svg)](https://golang.org)
[![Vue](https://img.shields.io/badge/Vue-3.5+-green.svg)](https://vuejs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/Lautloserspieler/JarvisCore?style=social)](https://github.com/Lautloserspieler/JarvisCore)


A modern AI assistant with holographic UI and **fully local llama.cpp inference**

[ğŸ‡©ğŸ‡ª Deutsche Version](./README.md) | [ğŸ“š Docs](./docs/) | [â“ FAQ](./FAQ.md) | [ğŸ”’ Security](./SECURITY.md)

</div>

---

## ğŸš€ Quickstart

- **Pinokio (recommended)**: [PINOKIO.md](./PINOKIO.md)
- **Manual quickstart**: [README_QUICKSTART.md](./README_QUICKSTART.md)
- **Troubleshooting**: [FAQ](./FAQ.md)

## âœ¨ Features

### ğŸ§  AI Engine
- âœ… **llama.cpp Local Inference** - Fully implemented and production-ready!
- âœ… **Automatic GPU Detection** - NVIDIA CUDA Support
- âœ… **7 GGUF Models** - Mistral, Qwen, DeepSeek, Llama and more
- âœ… **Chat with History** - Context-aware conversations
- âœ… **Up to 32K Context** - Long conversations possible
- âœ… **System Prompts** - Configurable JARVIS personality

### ğŸ¨ Frontend (Vue 3)
- âœ… **Holographic UI** - Stunning JARVIS-inspired user interface
- âœ… **Real-time Chat** - WebSocket-based live communication
- âœ… **Voice Interface** - Voice input with visual feedback
- âœ… **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Settings
- âœ… **Model Management** - Download and manage AI models
- âœ… **Plugin System** - Weather, Timer, Notes, News and more
- âœ… **Responsive Design** - Works on all screen sizes
- âœ… **Dark Theme** - Cyberpunk aesthetic with glowing effects

### ğŸš€ Backend (Python + FastAPI)
- âœ… **FastAPI Server** - High-performance Python backend
- âœ… **llama.cpp Integration** - Native GGUF model inference
- âœ… **WebSocket Support** - Real-time communication
- âœ… **RESTful API** - Complete REST endpoints
- âœ… **Plugin System** - Extensible architecture
- âœ… **Memory Storage** - Conversation history & context

---

## ğŸ’» Requirements

- **Python 3.11+** - [python.org](https://python.org)
- **Node.js 18+** - [nodejs.org](https://nodejs.org)
- **Git** - [git-scm.com](https://git-scm.com)
- **(Optional)** NVIDIA GPU with CUDA for accelerated inference

---

## ğŸš€ Installation & Setup

### Step 1: Clone Repository

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore
```

### Step 2: Install Base Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: llama.cpp Setup (ğŸ†• Automatic!)

**NEW:** Automatic GPU detection and optimal installation!

```bash
cd backend
python setup_llama.py
```

**The script automatically detects:**
- âœ… NVIDIA GPU â†’ Installs with CUDA Support (30-50 tok/s)
- âœ… AMD GPU â†’ Recommends CPU version (see below)
- âœ… No GPU â†’ Installs CPU version (5-10 tok/s)

**Example Output:**
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   JARVIS Core - llama.cpp Setup Script              â”‚
â”‚      Automatic GPU Detection & Install               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[INFO] System: Windows AMD64
[INFO] Python: 3.11.5
[INFO] Detecting GPU...
[INFO] NVIDIA GPU detected!

Installing llama-cpp-python with NVIDIA CUDA support

âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…

[SUCCESS] llama-cpp-python installed successfully!
[INFO] GPU Mode: NVIDIA CUDA
[INFO] You can now run: python main.py
```

### Step 4: Frontend Dependencies

```bash
cd frontend
npm install
cd ..
```

### Step 5: Start JARVIS

```bash
python main.py
```

**That's it!** The `main.py` script:
- âœ… Automatically starts backend & frontend
- âœ… Opens browser at http://localhost:5050
- âœ… Backend runs on http://localhost:5050

---

## ğŸ® Quick Start Alternative

### One-Liner Installation (Recommended)

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git && cd JarvisCore && pip install -r requirements.txt && cd backend && python setup_llama.py && cd ../frontend && npm install && cd .. && python main.py
```

---

## ğŸŒ Access Points

After starting, you can access:

- ğŸ¨ **Frontend UI**: http://localhost:5050
- ğŸ”§ **Backend API**: http://localhost:5050
- ğŸ“š **API Documentation**: http://localhost:5050/docs
- ğŸ”Œ **WebSocket**: ws://localhost:5050/ws

---

## ğŸ§  llama.cpp Local Inference

**NEW in v1.1.0** - Production-ready with automatic GPU detection!

### Features
- ğŸš€ **GPU Acceleration** - CUDA automatically detected
- ğŸ¯ **GGUF Support** - All llama.cpp-compatible models
- ğŸ’¬ **Chat Mode** - History support with up to 32K context
- âš¡ **Performance** - 30-50 tokens/sec (NVIDIA), 5-10 tokens/sec (CPU)

### GPU Support

| GPU Type | Support | Installation | Performance | Recommendation |
|---------|---------|--------------|-------------|----------------|
| **NVIDIA** | âœ… CUDA | Automatic | âš¡âš¡âš¡ 30-50 tok/s | â­ Recommended |
| **AMD** | âš ï¸ ROCm | Complex | âš¡âš¡âš¡ 25-40 tok/s | In Development ğŸ‘‰ **Use CPU Version** |
| **Intel Arc** | ğŸ”„ oneAPI | Coming Soon | âš¡âš¡ 20-35 tok/s | In Development |
| **CPU** | âœ… Standard | Automatic | âš¡ 5-10 tok/s | âœ… Works |

#### ğŸ’¡ Note for AMD GPU Users:

**ROCm setup is complex and requires:**
- Visual Studio Build Tools
- ROCm SDK Installation (~5 GB)
- Specific driver versions
- Multiple restarts
- Complicated path configuration

**ğŸ‘‰ Recommendation: Use the CPU version!**
```bash
python setup_llama.py
# Select Option 3: CPU Version
```

**CPU Version Advantages:**
- âœ… Ready to use immediately
- âœ… No complex configuration
- âœ… Stable and reliable
- âœ… 5-10 tokens/sec (sufficient for chat)
- âœ… Smaller models (3B) run smoothly

### Available Models

| Model | Size | Use Case | CPU Performance |
|-------|-------|----------|----------------|
| **Llama 3.2 3B** | ~2.0 GB | Small, fast | âš¡âš¡âš¡ 8-12 tok/s |
| **Phi-3 Mini** | ~2.3 GB | Compact, chat | âš¡âš¡âš¡ 7-10 tok/s |
| **Qwen 2.5 7B** | ~5.2 GB | Versatile | âš¡âš¡ 5-8 tok/s |
| **Mistral 7B Nemo** | ~7.5 GB | Code, technical | âš¡âš¡ 4-7 tok/s |
| **DeepSeek R1 8B** | ~6.9 GB | Analysis | âš¡ 3-6 tok/s |

**ğŸ‘‰ CPU Recommendation: Use Llama 3.2 3B or Phi-3 Mini for best performance!**

---

## ğŸ”§ Manual llama.cpp Installation

If the automatic script doesn't work:

### NVIDIA GPU (CUDA)

```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

### CPU Only (Recommended for AMD)

```bash
cd backend
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### AMD GPU (ROCm) - For Experts Only

âš ï¸ **Warning:** Very complex! Only recommended for experienced users.

1. **Install ROCm** (~5 GB): https://rocm.docs.amd.com/
2. **Install Visual Studio Build Tools**
3. **Restart required**
4. **Then:**
```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_HIPBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

---

## ğŸ“¦ Model Download System

JARVIS Core uses an **Ollama-inspired download system**:

### Features
- ğŸ”„ **Multi-Registry Support** - HuggingFace, Ollama, Custom URLs
- ğŸ“¦ **Resume Downloads** - Interrupted downloads can be resumed
- âœ… **SHA256 Verification** - Automatic integrity checking
- ğŸ“Š **Live Progress** - Speed, ETA, progress bar
- ğŸ” **HuggingFace Token** - Support for private repositories

### Managing Models

1. **Start JARVIS**: `python main.py`
2. **Open Web UI**: http://localhost:5050
3. **Models Tab**: Navigate to model management
4. **Download Model**: Click "Download" â†’ Select quantization
5. **Load Model**: Click "Load" on downloaded model
6. **Start Chat**: Go to "Chat" tab and type

More info: [docs/LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## ğŸ”Œ Plugin System

**NEW in v1.1.0** - Extensible plugin architecture!

### Available Plugins

| Plugin | Description | API Key |
|--------|-------------|----------|
| â˜€ï¸ **Weather** | OpenWeatherMap Integration | âœ… Required |
| â° **Timer** | Timers & Reminders | âŒ Not needed |
| ğŸ“ **Notes** | Quick Notes | âŒ Not needed |
| ğŸ“° **News** | RSS News Feeds | âŒ Not needed |

### Activating Plugins

1. Open **Plugins Tab** in the UI
2. Click **"Activate"** on the desired plugin
3. If API key required â†’ Modal opens automatically
4. Enter API key â†’ Stored securely in `config/settings.json`
5. Plugin activated! âœ…

---

## ğŸ“ Project Structure

```
JarvisCore/
â”œâ”€â”€ main.py                 # ğŸš€ Unified Launcher
â”œâ”€â”€ requirements.txt        # ğŸ“¦ Python Dependencies
â”œâ”€â”€ core/                   # ğŸ§  Core Python Modules
â”‚   â”œâ”€â”€ llama_inference.py # llama.cpp Engine
â”‚   â”œâ”€â”€ model_downloader.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/                # ğŸ”§ Python/FastAPI Backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ setup_llama.py     # ğŸ†• Auto GPU Setup
â”‚   â”œâ”€â”€ plugin_manager.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/               # ğŸ¨ Vue 3 Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ plugins/                # ğŸ”Œ Plugin System
â”‚   â”œâ”€â”€ weather_plugin.py
â”‚   â”œâ”€â”€ timer_plugin.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/llm/             # ğŸ“¦ GGUF Models
â”œâ”€â”€ config/                 # âš™ï¸ Configuration
â”œâ”€â”€ data/                   # ğŸ—„ï¸ User Data
â”œâ”€â”€ docs/                   # ğŸ“š Documentation
â””â”€â”€ README.md
```

---

## ğŸ› Troubleshooting

### Problem: GPU not detected

```bash
# Check GPU status
nvidia-smi  # NVIDIA

# Reinstall llama.cpp
cd backend
python setup_llama.py
```

### Problem: Port already in use

```bash
# Windows
netstat -ano | findstr :5050
netstat -ano | findstr :5050

# Linux/Mac
lsof -i :5050
lsof -i :5050
```

### Problem: Module not found

```bash
pip install -r requirements.txt
cd frontend && npm install
```

### Problem: AMD GPU - ROCm Installation too complex

**Solution: Use CPU version!**
```bash
cd backend
python setup_llama.py
# Select Option 3
```

More help: [â“ FAQ](./FAQ.md) | [ğŸ“š Troubleshooting](./docs/TROUBLESHOOTING.md)

---

## ğŸ¯ Roadmap

### âœ… v1.1.0 (Current) - December 2025
- âœ… Vue 3 Frontend
- âœ… Production-ready llama.cpp
- âœ… Automatic GPU Detection
- âœ… Plugin System with API Key Management
- âœ… Model Download System

### ğŸ”„ v1.2.0 - Q1 2026
- Voice Input (Whisper)
- Voice Output (XTTS v2)
- Desktop App (Wails)
- Enhanced Memory System
- Docker Support

### ğŸ“‹ v2.0.0 - Q2 2026
- RAG Implementation
- Vector Database
- Multi-User Support
- Cloud Deployment

See also: [ğŸ“‹ CHANGELOG](./CHANGELOG.md) for detailed release notes

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### Quick Start for Contributors

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'feat: Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Create a Pull Request

---

## ğŸ“„ License

**Apache License 2.0** with additional commercial restriction.

Full license: [LICENSE](./LICENSE)

---

## ğŸ™ Acknowledgments

- Inspired by JARVIS from Iron Man
- Built with [Vue 3](https://vuejs.org/)
- Backend with [FastAPI](https://fastapi.tiangolo.com/)
- Local inference with [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

## ğŸ“š Additional Documentation

- [Quick Start Guide](docs/README_QUICKSTART.md)
- [Architecture Overview](docs/ARCHITECTURE.md)
- [LLM Download System](docs/LLM_DOWNLOAD_SYSTEM.md)
- [Performance Guide](docs/PERFORMANCE.md)
- [Deployment Guide](DEPLOYMENT.md)
- [Contributing Guidelines](CONTRIBUTING.md)
- [FAQ](FAQ.md)
- [Changelog](CHANGELOG.md)

---

<div align="center">

**Made with â¤ï¸ by Lautloserspieler**

*"Sometimes you gotta run before you can walk."* - Tony Stark

**Version:** 1.1.0 | **Release:** January 02, 2026

[â­ Star us on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [ğŸ› Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [ğŸ’¡ Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>
