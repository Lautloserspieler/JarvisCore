#!/usr/bin/env python3
"""
Async LLM Wrapper f√ºr J.A.R.V.I.S.
Non-blocking LLM Inference mit Batching & Performance Monitoring
"""

import asyncio
import time
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
from collections import deque

from core.llm_manager import LLMManager
from utils.performance_monitor import perf_monitor, measure_async
from utils.logger import Logger


class AsyncLLMWrapper:
    """
    Async Wrapper f√ºr LLMManager mit:
    - Non-blocking Inference
    - Request Batching
    - Performance Monitoring
    - Connection Pooling
    """

    def __init__(self, llm_manager: LLMManager, max_workers: int = 2):
        self.llm_manager = llm_manager
        self.logger = Logger()
        
        # Thread Pool f√ºr Blocking Operations
        self.executor = ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="llm_worker"
        )
        
        # Batching Queue
        self.batch_queue: deque = deque()
        self.batch_size = 4
        self.batch_timeout = 0.1  # 100ms
        self._batch_task: Optional[asyncio.Task] = None
        self._batch_lock = asyncio.Lock()
        
        # Stats
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "batch_processed": 0,
        }

    async def start_batch_processor(self):
        """Startet Batch Processor Loop"""
        if self._batch_task is None or self._batch_task.done():
            self._batch_task = asyncio.create_task(self._batch_processor_loop())
            self.logger.info("üöÄ Async LLM Batch Processor gestartet")

    async def stop_batch_processor(self):
        """Stoppt Batch Processor"""
        if self._batch_task and not self._batch_task.done():
            self._batch_task.cancel()
            try:
                await self._batch_task
            except asyncio.CancelledError:
                pass
            self.logger.info("üõë Async LLM Batch Processor gestoppt")

    @measure_async("llm_generate_async")
    async def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model_key: Optional[str] = None,
        use_case: Optional[str] = None,
        enable_cache: bool = True,
    ) -> Optional[str]:
        """Async Response Generation
        
        Non-blocking Alternative zu llm_manager.generate_response()
        """
        self.stats["total_requests"] += 1
        perf_monitor.increment_counter("llm_requests_total")
        
        # Cache Check (sync, aber schnell)
        if enable_cache:
            cache_key = self.llm_manager._build_cache_key(
                model_key or "default",
                prompt,
                system_prompt,
                temperature or 0.65,
                max_tokens or 256
            )
            cached = self.llm_manager._get_cached_response(cache_key)
            if cached:
                self.stats["cache_hits"] += 1
                perf_monitor.increment_counter("llm_cache_hits")
                perf_monitor.record_event("llm_cache_hit", 1.0)
                return cached
        
        # Non-blocking Execution
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.llm_manager.generate_response,
            prompt,
            system_prompt,
            temperature,
            max_tokens,
            model_key,
            use_case,
            None,  # task_hint
            enable_cache,
        )
        
        perf_monitor.increment_counter("llm_requests_completed")
        return result

    async def generate_with_batching(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model_key: Optional[str] = None,
    ) -> str:
        """Response Generation mit Batching
        
        Sammelt Requests und verarbeitet sie in Batches
        +300% Throughput bei 4+ parallel requests
        """
        future = asyncio.Future()
        
        async with self._batch_lock:
            self.batch_queue.append({
                "prompt": prompt,
                "system_prompt": system_prompt,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "model_key": model_key,
                "future": future,
                "timestamp": time.time(),
            })
            
            # Trigger Batch Processing wenn Queue voll
            if len(self.batch_queue) >= self.batch_size:
                asyncio.create_task(self._process_batch())
        
        return await future

    async def _batch_processor_loop(self):
        """Background Loop f√ºr Batch Processing"""
        while True:
            try:
                await asyncio.sleep(self.batch_timeout)
                
                async with self._batch_lock:
                    if self.batch_queue:
                        await self._process_batch()
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Batch Processor Error: {e}")

    @measure_async("llm_batch_process")
    async def _process_batch(self):
        """Verarbeitet Batch von Requests"""
        if not self.batch_queue:
            return
        
        # Extract Batch
        batch_items = []
        for _ in range(min(self.batch_size, len(self.batch_queue))):
            if self.batch_queue:
                batch_items.append(self.batch_queue.popleft())
        
        if not batch_items:
            return
        
        self.stats["batch_processed"] += 1
        perf_monitor.increment_counter("llm_batches_processed")
        perf_monitor.record_event("llm_batch_size", len(batch_items))
        
        # Process parallel (ein Thread pro Item)
        tasks = [
            self.generate_response(
                item["prompt"],
                item["system_prompt"],
                item["temperature"],
                item["max_tokens"],
                item["model_key"],
                enable_cache=True,
            )
            for item in batch_items
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Set Futures
        for item, result in zip(batch_items, results):
            if isinstance(result, Exception):
                item["future"].set_exception(result)
            else:
                item["future"].set_result(result)

    @measure_async("llm_load_model_async")
    async def load_model(self, model_key: str) -> bool:
        """Async Model Loading
        
        Non-blocking Alternative zu llm_manager.load_model()
        """
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.llm_manager.load_model,
            model_key
        )
        
        if result:
            perf_monitor.record_event("llm_model_loaded", 1.0, {"model": model_key})
        
        return result

    async def get_stats(self) -> Dict[str, Any]:
        """Async Stats Abruf"""
        return {
            "wrapper_stats": self.stats,
            "llm_stats": self.llm_manager.get_model_status(),
            "performance": perf_monitor.get_stats("llm_generate_async"),
            "batch_performance": perf_monitor.get_stats("llm_batch_process"),
        }

    def shutdown(self):
        """Cleanup"""
        self.executor.shutdown(wait=True)
        self.logger.info("üõ†Ô∏è Async LLM Wrapper beendet")


# Convenience Factory
def create_async_llm(llm_manager: LLMManager, max_workers: int = 2) -> AsyncLLMWrapper:
    """Factory f√ºr AsyncLLMWrapper
    
    Usage:
        llm_manager = LLMManager()
        async_llm = create_async_llm(llm_manager)
        
        # Async Usage
        response = await async_llm.generate_response("Hello!")
        
        # Batching
        await async_llm.start_batch_processor()
        response = await async_llm.generate_with_batching("Batch request")
    """
    return AsyncLLMWrapper(llm_manager, max_workers)
