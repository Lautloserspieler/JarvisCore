# ü§ñ JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

Ein moderner KI-Assistent mit holographischer UI und **vollst√§ndig lokaler llama.cpp Inferenz**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18.3+-cyan.svg)](https://reactjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.8+-blue.svg)](https://typescriptlang.org)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

[üá¨üáß English Version](./README_GB.md)

</div>

---

## ‚ú® Features

### üß† KI-Engine (NEU v1.0.1!)
- ‚úÖ **llama.cpp Lokale Inferenz** - Vollst√§ndig implementiert und funktionsf√§hig!
- ‚úÖ **GPU-Acceleration** - Automatische CUDA-Erkennung (30-50 tok/s)
- ‚úÖ **4 GGUF-Modelle** - Mistral, Qwen, DeepSeek, Llama 2 (Q4_K_M)
- ‚úÖ **Chat mit History** - Kontext-bewusste Konversationen
- ‚úÖ **Bis 32K Context** - Lange Konversationen m√∂glich
- ‚úÖ **System-Prompts** - JARVIS-Pers√∂nlichkeit konfigurierbar

### üé® Frontend
- ‚úÖ **Holographische UI** - Beeindruckende JARVIS-inspirierte Benutzeroberfl√§che
- ‚úÖ **Echtzeit-Chat** - WebSocket-basierte Live-Kommunikation mit **echter AI**
- ‚úÖ **Sprach-Interface** - Visuelle Voice-Input-R√ºckmeldung
- ‚úÖ **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Plugins, Logs, Settings
- ‚úÖ **Model-Management** - Download und Verwaltung von KI-Modellen (Ollama-Style)
- ‚úÖ **Download-Queue** - Live-Progress-Tracking mit Speed & ETA
- ‚úÖ **Responsive Design** - Funktioniert auf allen Bildschirmgr√∂√üen
- ‚úÖ **Dark Theme** - Cyberpunk-√Ñsthetik mit leuchtenden Effekten

### üöÄ Backend
- ‚úÖ **FastAPI Server** - Hochperformanter Async-API-Server
- ‚úÖ **llama.cpp Integration** - Native GGUF-Model-Inferenz
- ‚úÖ **WebSocket Support** - Echtzeitkommunikation in beide Richtungen
- ‚úÖ **RESTful API** - Vollst√§ndige REST-Endpunkte
- ‚úÖ **LLM Download-System** - Ollama-inspiriertes Multi-Registry-System
- ‚úÖ **Model Management** - Laden/Entladen von Modellen zur Laufzeit
- ‚úÖ **Plugin System** - Erweiterbare Architektur
- ‚úÖ **Memory Storage** - Konversationshistorie & Kontext
- ‚úÖ **System Logs** - Umfassendes Logging

---

## üöÄ Schnellstart

### Voraussetzungen
- Python 3.8+
- Node.js 18+
- npm oder yarn
- **(Optional)** NVIDIA GPU mit CUDA f√ºr beschleunigte Inferenz

### Installation

```bash
# Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Alles mit einem Befehl starten!
python main.py
```

Das war's! Das einheitliche `main.py` Script wird:
1. ‚úÖ Alle Anforderungen pr√ºfen
2. ‚úÖ Fehlende Abh√§ngigkeiten installieren (inkl. llama-cpp-python)
3. ‚úÖ Backend-Server starten
4. ‚úÖ Frontend-Dev-Server starten
5. ‚úÖ Browser automatisch √∂ffnen

---

## üåê Zugriffspunkte

Nach dem Start erreichst du:

- üé® **Frontend UI**: http://localhost:5000
- üîß **Backend API**: http://localhost:5050
- üìö **API-Dokumentation**: http://localhost:5050/docs
- üîå **WebSocket**: ws://localhost:5050/ws

---

## üß† llama.cpp Lokale Inferenz

**NEU in v1.0.1** - Vollst√§ndig implementiert und production-ready!

### Features
- üöÄ **GPU-Acceleration** - CUDA automatisch erkannt, alle Layers auf GPU
- üéØ **GGUF-Support** - Alle llama.cpp-kompatiblen Modelle
- üí¨ **Chat-Modus** - History-Support mit bis zu 32K Context
- ‚ö° **Performance** - 30-50 tokens/sec (GPU), 5-10 tokens/sec (CPU)
- üßµ **Thread-Safe** - Parallele Requests m√∂glich
- üíæ **Memory-Efficient** - Automatisches Model Loading/Unloading

### Verf√ºgbare Modelle

| Model | Gr√∂√üe | Use Case | Performance |
|-------|-------|----------|-------------|
| **Mistral 7B Nemo** | ~7.5 GB | Code, technische Details | ‚ö°‚ö°‚ö° |
| **Qwen 2.5 7B** | ~5.2 GB | Vielseitig, multilingual | ‚ö°‚ö°‚ö° |
| **DeepSeek R1 8B** | ~6.9 GB | Analysen, Reasoning | ‚ö°‚ö° |
| **Llama 2 7B** | ~4.0 GB | Kreativ, Chat | ‚ö°‚ö°‚ö° |

### Verwendung

```python
from core.llama_inference import llama_runtime

# Modell laden
llama_runtime.load_model(
    model_path="models/llm/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    model_name="mistral",
    n_ctx=8192,        # 8K Context-Window
    n_gpu_layers=-1    # Alle Layers auf GPU
)

# Chat mit History
result = llama_runtime.chat(
    message="Erkl√§re mir Quantencomputing",
    history=[
        {"role": "user", "content": "Hallo!"},
        {"role": "assistant", "content": "Hallo! Wie kann ich dir helfen?"}
    ],
    system_prompt="Du bist JARVIS, ein hilfreicher deutscher KI-Assistent.",
    temperature=0.7,
    max_tokens=512
)

print(result['text'])  # Echte AI-Antwort!
print(f"{result['tokens_per_second']:.1f} tok/s")  # Performance-Tracking
```

---

## üì¶ Model-Download-System

JARVIS Core nutzt ein **Ollama-inspiriertes Download-System** f√ºr KI-Modelle:

### Features
- üîÑ **Multi-Registry-Support** - HuggingFace, Ollama, Custom URLs
- üì¶ **Resume-Downloads** - Unterbrochene Downloads werden fortgesetzt
- ‚úÖ **SHA256-Verifizierung** - Automatische Integrit√§tspr√ºfung
- üìä **Live-Progress** - Download-Speed, ETA, Fortschrittsbalken
- üéØ **Quantization-Varianten** - Q4_K_M, Q5_K_M, Q6_K, Q8_0
- üîê **HuggingFace Token** - Support f√ºr private Repositories

### Models verwalten

1. **Web-UI √∂ffnen**: http://localhost:5000
2. **Models-Tab**: Navigation zur Model-Verwaltung
3. **Model downloaden**: 
   - Klick auf "Download" bei gew√ºnschtem Modell
   - W√§hle Quantization-Variante (z.B. Q4_K_M)
   - Download startet automatisch
4. **Model laden**:
   - Klick "Load" bei heruntergeladenem Modell
   - Warte auf "‚úì Model loaded successfully"
5. **Chat starten**:
   - Gehe zu "Chat" Tab
   - Schreibe Nachricht
   - Erhalte **echte AI-Antwort** mit llama.cpp!

Weitere Infos: [LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## üìÅ Projektstruktur

```
JarvisCore/
‚îú‚îÄ‚îÄ main.py                 # üöÄ Einheitliches Startup-Script
‚îú‚îÄ‚îÄ core/                   # üß† Core-Module
‚îÇ   ‚îú‚îÄ‚îÄ llama_inference.py # ‚≠ê NEU: llama.cpp Inference Engine
‚îÇ   ‚îú‚îÄ‚îÄ llm_manager.py     # LLM-Management
‚îÇ   ‚îú‚îÄ‚îÄ model_downloader.py # Download-Engine
‚îÇ   ‚îú‚îÄ‚îÄ model_registry.py   # Multi-Registry
‚îÇ   ‚îú‚îÄ‚îÄ model_manifest.py   # Metadata-Management
‚îÇ   ‚îî‚îÄ‚îÄ ...                # Weitere Module
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI-Server mit llama.cpp
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Python-Abh√§ngigkeiten
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/    # React-Komponenten
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/       # shadcn/ui Komponenten
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabs/     # Tab-Komponenten
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/   # Model-Management-Komponenten
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.tsx     # Haupt-Komponenten
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/      # API & WebSocket Services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/         # Custom React Hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/         # Seiten-Komponenten
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lib/           # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ models/llm/            # üì¶ GGUF-Modelle hier ablegen
‚îú‚îÄ‚îÄ docs/                   # üìö Dokumentation
‚îÇ   ‚îú‚îÄ‚îÄ LLM_DOWNLOAD_SYSTEM.md
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Entwicklung

### Manueller Start (Development-Modus)

#### Backend
```bash
cd backend
pip install -r requirements.txt
python main.py
```

#### Frontend
```bash
cd frontend
npm install
npm run dev
```

---

## üîå API-Endpunkte

### Chat
- `WS /ws` - WebSocket-Chat mit llama.cpp AI-Responses
- `GET /api/chat/sessions` - Alle Chat-Sessions abrufen
- `POST /api/chat/sessions` - Neue Session erstellen

### Models
- `GET /api/models` - Alle Modelle auflisten
- `GET /api/models/active` - Aktives Modell abrufen
- `POST /api/models/{id}/load` - Modell laden (llama.cpp)
- `POST /api/models/unload` - Modell entladen
- `POST /api/models/download` - Model-Download starten
- `GET /api/models/download/progress` - Download-Progress (SSE)
- `POST /api/models/cancel` - Download abbrechen
- `DELETE /api/models/delete` - Modell l√∂schen

### System
- `GET /api/health` - Health-Check mit llama.cpp Status
- `GET /api/logs` - System-Logs abrufen

---

## üé® Technologie-Stack

### KI & Inferenz
- **llama.cpp** - Native GGUF-Model-Inferenz
- **llama-cpp-python** - Python-Bindings f√ºr llama.cpp
- **CUDA** - GPU-Acceleration (optional)

### Frontend
- **Framework**: React 18 + TypeScript
- **Build Tool**: Vite
- **UI Library**: shadcn/ui (Radix UI + Tailwind CSS)
- **Routing**: React Router
- **State Management**: TanStack Query
- **WebSocket**: Native WebSocket API
- **Icons**: Lucide React

### Backend
- **Framework**: FastAPI
- **Server**: Uvicorn
- **WebSocket**: FastAPI WebSocket
- **Type Safety**: Pydantic
- **HTTP Client**: httpx (f√ºr Downloads)

---

## üéØ Features-Roadmap

### ‚úÖ Aktuell (v1.0.1) - 16. Dezember 2025
- ‚úÖ **llama.cpp Lokale Inferenz** - PRODUCTION READY!
- ‚úÖ GPU-Acceleration (CUDA)
- ‚úÖ Chat mit History-Support
- ‚úÖ 4 GGUF-Modelle vorkonfiguriert
- ‚úÖ Model-Download-System (Ollama-Style)
- ‚úÖ Live-Progress-Tracking
- ‚úÖ Multi-Registry-Support
- ‚úÖ WebSocket-Chat mit echter AI
- ‚úÖ Basis-UI mit allen Tabs

### Geplant (v1.2.0) - Q1 2026
- üîÑ Voice Input (Whisper STT)
- üîÑ Voice Output (XTTS v2 TTS)
- üîÑ Model-Switching ohne Neustart
- üîÑ Bessere Memory-Integration
- üîÑ Performance-Optimierungen

### Zukunft (v2.0.0) - Q2 2026
- üìã RAG (Retrieval-Augmented Generation)
- üìã Vector-Database (ChromaDB/FAISS)
- üìã Multi-User-Support
- üìã Benutzer-Authentifizierung
- üìã Cloud-Deployment (AWS/GCP)
- üìã Mobile App
- üìã Advanced Plugin-Marketplace

---

## ü§ù Mitwirken

Beitr√§ge sind willkommen! Bitte f√ºhle dich frei, einen Pull Request einzureichen.

---

## üìÑ Lizenz

**Apache License 2.0** mit zus√§tzlicher kommerzieller Einschr√§nkung.

Dieses Projekt ist unter der Apache License 2.0 lizenziert mit folgender **zus√§tzlicher Einschr√§nkung**:

> **Kommerzielle Nutzung, Verkauf oder Weitervertrieb dieser Software ist ohne vorherige schriftliche Genehmigung des Copyright-Inhabers untersagt.**

Diese Einschr√§nkung gilt nur f√ºr den originalen J.A.R.V.I.S. Quellcode und zugeh√∂rige Assets von Lautloserspieler. Alle enthaltenen Drittanbieter-Komponenten (wie Sprachmodelle, Speech-Libraries oder externe APIs) unterliegen ihren jeweiligen Lizenzen.

Vollst√§ndige Lizenz: [LICENSE](./LICENSE)

---

## üôè Danksagungen

- Inspiriert von JARVIS aus Iron Man
- Gebaut mit [shadcn/ui](https://ui.shadcn.com/)
- Powered by [FastAPI](https://fastapi.tiangolo.com/)
- Lokale Inferenz mit [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Download-System inspiriert von [Ollama](https://ollama.ai/)

---

## üìö Weitere Dokumentation

- [LLM Download-System](./docs/LLM_DOWNLOAD_SYSTEM.md) - Detaillierte Dokumentation des Download-Systems
- [Architektur](./docs/ARCHITECTURE.md) - System-Architektur-√úbersicht
- [Implementation Status](./IMPLEMENTATION_STATUS.md) - Feature-Status und Roadmap
- [Changelog](./docs/CHANGELOG.md) - Versions-Historie
- [Backend-API](./backend/README.md) - Backend-spezifische Dokumentation

---

<div align="center">

**Erstellt mit ‚ù§Ô∏è vom JARVIS-Team**

*"Manchmal muss man rennen, bevor man gehen kann."* - Tony Stark

**Version:** 1.0.1 | **Stand:** 16. Dezember 2025, 11:15 CET

</div>
