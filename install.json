{
  "run": [
    {
      "method": "input",
      "params": {
        "title": "GPU Backend",
        "description": "WÃ¤hle deinen GPU-Typ:",
        "fields": [
          {
            "key": "gpu",
            "type": "select",
            "title": "Beschleunigung",
            "options": [
              { "title": "CPU", "value": "cpu" },
              { "title": "NVIDIA CUDA", "value": "cuda" },
              { "title": "AMD ROCm", "value": "rocm" },
              { "title": "Apple Metal", "value": "metal" }
            ],
            "default": "cuda"
          }
        ]
      }
    },
    {
      "method": "local.set",
      "params": {
        "gpu": "{{input.gpu}}"
      }
    },
    {
      "method": "shell.run",
      "params": { "message": "python -m venv venv" }
    },
    {
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": [
          "python -m pip install -U pip setuptools wheel",
          "pip install numpy==1.26.4 cython",
          "pip install -e \".[tts]\""
        ]
      }
    },
    {
      "when": "{{local.gpu === 'cpu'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu === 'cuda'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "build": true,
        "env": { "FORCE_CMAKE": "1", "CMAKE_ARGS": "-DGGML_CUDA=on" },
        "message": [
          "pip uninstall -y llama-cpp-python",
          "pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
        ]
      }
    },
    {
      "when": "{{local.gpu === 'rocm'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": { "FORCE_CMAKE": "1", "CMAKE_ARGS": "-DGGML_HIPBLAS=on" },
        "message": [
          "pip uninstall -y llama-cpp-python",
          "pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
        ]
      }
    },
    {
      "when": "{{local.gpu === 'metal'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": { "FORCE_CMAKE": "1", "CMAKE_ARGS": "-DGGML_METAL=on" },
        "message": [
          "pip uninstall -y llama-cpp-python",
          "pip install --no-cache-dir --force-reinstall --no-binary llama-cpp-python llama-cpp-python"
        ]
      }
    },
    {
      "method": "shell.run",
      "params": {
        "path": "frontend",
        "message": [
          "npm install",
          "npm run build"
        ]
      }
    },
    {
      "method": "notify",
      "params": {
        "html": "Installation abgeschlossen.<br><b>GPU:</b> {{local.gpu}}<br>Klicke 'Start' zum Starten."
      }
    }
  ]
}