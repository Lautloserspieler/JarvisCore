{
  "run": [
    {
      "method": "input",
      "params": {
        "title": "GPU Backend",
        "description": "W√§hle den Backend-Typ f√ºr llama-cpp-python.",
        "fields": [
          {
            "key": "gpu_type",
            "type": "select",
            "title": "GPU-Beschleunigung",
            "options": [
              { "title": "üîµ CPU", "value": "cpu" },
              { "title": "üü¢ NVIDIA CUDA", "value": "cuda" },
              { "title": "üü† AMD ROCm (Linux)", "value": "rocm" },
              { "title": "üçé Apple Metal (macOS)", "value": "metal" }
            ],
            "default": "cpu"
          }
        ]
      }
    },
    {
      "method": "local.set",
      "params": {
        "gpu_type": "{{input.gpu_type}}"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": [
          "python -m pip install -U pip setuptools wheel",
          "pip install -e \".[tts]\""
        ]
      }
    },
    {
      "when": "{{local.gpu_type === 'cpu'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu_type === 'cuda' && platform === 'win32'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "build": true,
        "env": {
          "CMAKE_ARGS": "-DGGML_CUDA=on",
          "FORCE_CMAKE": "1"
        },
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu_type === 'cuda' && platform !== 'win32'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "CMAKE_ARGS": "-DGGML_CUDA=on",
          "FORCE_CMAKE": "1"
        },
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu_type === 'rocm' && platform === 'linux'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "CMAKE_ARGS": "-DGGML_HIPBLAS=on",
          "FORCE_CMAKE": "1"
        },
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "when": "{{local.gpu_type === 'metal' && platform === 'darwin'}}",
      "method": "shell.run",
      "params": {
        "venv": "venv",
        "env": {
          "CMAKE_ARGS": "-DGGML_METAL=on",
          "FORCE_CMAKE": "1"
        },
        "message": "pip install -U --no-cache-dir llama-cpp-python"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "path": "frontend",
        "message": "npm install"
      }
    },
    {
      "method": "notify",
      "params": {
        "html": "Installation abgeschlossen!<br><br><b>GPU-Typ:</b> {{local.gpu_type === 'cuda' ? 'NVIDIA CUDA üü¢' : local.gpu_type === 'rocm' ? 'AMD ROCm üü†' : local.gpu_type === 'metal' ? 'Apple Metal üçé' : 'CPU üîµ'}}<br><br>Klicke 'Start', um JARVIS zu starten."
      }
    }
  ]
}