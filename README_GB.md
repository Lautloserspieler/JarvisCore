# ü§ñ JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

A modern AI assistant with holographic UI and **fully local llama.cpp inference**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18.3+-cyan.svg)](https://reactjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.8+-blue.svg)](https://typescriptlang.org)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

[üá©üá™ Deutsche Version](./README.md)

</div>

---

## ‚ú® Features

### üß† AI Engine (NEW v1.0.1!)
- ‚úÖ **llama.cpp Local Inference** - Fully implemented and production-ready!
- ‚úÖ **GPU Acceleration** - Automatic CUDA detection (30-50 tok/s)
- ‚úÖ **4 GGUF Models** - Mistral, Qwen, DeepSeek, Llama 2 (Q4_K_M)
- ‚úÖ **Chat with History** - Context-aware conversations
- ‚úÖ **Up to 32K Context** - Long conversations possible
- ‚úÖ **System Prompts** - Configurable JARVIS personality

### üé® Frontend
- ‚úÖ **Holographic UI** - Stunning JARVIS-inspired interface
- ‚úÖ **Real-time Chat** - WebSocket-based live communication with **real AI**
- ‚úÖ **Voice Interface** - Visual voice input feedback
- ‚úÖ **Multi-tab Navigation** - Chat, Dashboard, Memory, Models, Plugins, Logs, Settings
- ‚úÖ **Model Management** - Download and manage AI models (Ollama-style)
- ‚úÖ **Download Queue** - Live progress tracking with speed & ETA
- ‚úÖ **Responsive Design** - Works on all screen sizes
- ‚úÖ **Dark Theme** - Cyberpunk aesthetic with glowing effects

### üöÄ Backend
- ‚úÖ **FastAPI Server** - High-performance async API
- ‚úÖ **llama.cpp Integration** - Native GGUF model inference
- ‚úÖ **WebSocket Support** - Real-time bidirectional communication
- ‚úÖ **RESTful API** - Complete REST endpoints
- ‚úÖ **LLM Download System** - Ollama-inspired multi-registry system
- ‚úÖ **Model Management** - Load/unload models at runtime
- ‚úÖ **Plugin System** - Extensible architecture
- ‚úÖ **Memory Storage** - Conversation history & context
- ‚úÖ **System Logs** - Comprehensive logging

---

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 18+
- npm or yarn
- **(Optional)** NVIDIA GPU with CUDA for accelerated inference

### Installation

```bash
# Clone the repository
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Start everything with one command!
python main.py
```

That's it! The unified `main.py` script will:
1. ‚úÖ Check all requirements
2. ‚úÖ Install missing dependencies (including llama-cpp-python)
3. ‚úÖ Start the backend server
4. ‚úÖ Start the frontend dev server
5. ‚úÖ Open your browser automatically

---

## üåê Access Points

Once started, you can access:

- üé® **Frontend UI**: http://localhost:5000
- üîß **Backend API**: http://localhost:5050
- üìö **API Documentation**: http://localhost:5050/docs
- üîå **WebSocket**: ws://localhost:5050/ws

---

## üß† llama.cpp Local Inference

**NEW in v1.0.1** - Fully implemented and production-ready!

### Features
- üöÄ **GPU Acceleration** - CUDA automatically detected, all layers on GPU
- üéØ **GGUF Support** - All llama.cpp-compatible models
- üí¨ **Chat Mode** - History support with up to 32K context
- ‚ö° **Performance** - 30-50 tokens/sec (GPU), 5-10 tokens/sec (CPU)
- üßµ **Thread-Safe** - Parallel requests possible
- üíæ **Memory-Efficient** - Automatic model loading/unloading

### Available Models

| Model | Size | Use Case | Performance |
|-------|------|----------|-------------|
| **Mistral 7B Nemo** | ~7.5 GB | Code, technical details | ‚ö°‚ö°‚ö° |
| **Qwen 2.5 7B** | ~5.2 GB | Versatile, multilingual | ‚ö°‚ö°‚ö° |
| **DeepSeek R1 8B** | ~6.9 GB | Analysis, reasoning | ‚ö°‚ö° |
| **Llama 2 7B** | ~4.0 GB | Creative, chat | ‚ö°‚ö°‚ö° |

### Usage

```python
from core.llama_inference import llama_runtime

# Load model
llama_runtime.load_model(
    model_path="models/llm/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    model_name="mistral",
    n_ctx=8192,        # 8K context window
    n_gpu_layers=-1    # All layers on GPU
)

# Chat with history
result = llama_runtime.chat(
    message="Explain quantum computing to me",
    history=[
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hello! How can I help you?"}
    ],
    system_prompt="You are JARVIS, a helpful AI assistant.",
    temperature=0.7,
    max_tokens=512
)

print(result['text'])  # Real AI response!
print(f"{result['tokens_per_second']:.1f} tok/s")  # Performance tracking
```

---

## üì¶ Model Download System

JARVIS Core uses an **Ollama-inspired download system** for AI models:

### Features
- üîÑ **Multi-Registry Support** - HuggingFace, Ollama, Custom URLs
- üì¶ **Resume Downloads** - Interrupted downloads are resumed
- ‚úÖ **SHA256 Verification** - Automatic integrity checking
- üìä **Live Progress** - Download speed, ETA, progress bar
- üéØ **Quantization Variants** - Q4_K_M, Q5_K_M, Q6_K, Q8_0
- üîê **HuggingFace Token** - Support for private repositories

### Managing Models

1. **Open Web UI**: http://localhost:5000
2. **Models Tab**: Navigate to model management
3. **Download Model**: 
   - Click "Download" on desired model
   - Select quantization variant (e.g., Q4_K_M)
   - Download starts automatically
4. **Load Model**:
   - Click "Load" on downloaded model
   - Wait for "‚úì Model loaded successfully"
5. **Start Chat**:
   - Go to "Chat" tab
   - Write message
   - Get **real AI response** with llama.cpp!

More info: [LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## üìÅ Project Structure

```
JarvisCore/
‚îú‚îÄ‚îÄ main.py                 # üöÄ Unified startup script
‚îú‚îÄ‚îÄ core/                   # üß† Core modules
‚îÇ   ‚îú‚îÄ‚îÄ llama_inference.py # ‚≠ê NEW: llama.cpp Inference Engine
‚îÇ   ‚îú‚îÄ‚îÄ llm_manager.py     # LLM management
‚îÇ   ‚îú‚îÄ‚îÄ model_downloader.py # Download engine
‚îÇ   ‚îú‚îÄ‚îÄ model_registry.py   # Multi-registry
‚îÇ   ‚îú‚îÄ‚îÄ model_manifest.py   # Metadata management
‚îÇ   ‚îî‚îÄ‚îÄ ...                # Additional modules
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI server with llama.cpp
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/    # React components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/       # shadcn/ui components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabs/     # Tab components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/   # Model management components
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.tsx     # Main components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/      # API & WebSocket services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/         # Custom React hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/         # Page components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lib/           # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ models/llm/            # üì¶ Place GGUF models here
‚îú‚îÄ‚îÄ docs/                   # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ LLM_DOWNLOAD_SYSTEM.md
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Development

### Manual Start (Development Mode)

#### Backend
```bash
cd backend
pip install -r requirements.txt
python main.py
```

#### Frontend
```bash
cd frontend
npm install
npm run dev
```

---

## üîå API Endpoints

### Chat
- `WS /ws` - WebSocket chat with llama.cpp AI responses
- `GET /api/chat/sessions` - Get all chat sessions
- `POST /api/chat/sessions` - Create new session

### Models
- `GET /api/models` - List all models
- `GET /api/models/active` - Get active model
- `POST /api/models/{id}/load` - Load model (llama.cpp)
- `POST /api/models/unload` - Unload model
- `POST /api/models/download` - Start model download
- `GET /api/models/download/progress` - Download progress (SSE)
- `POST /api/models/cancel` - Cancel download
- `DELETE /api/models/delete` - Delete model

### System
- `GET /api/health` - Health check with llama.cpp status
- `GET /api/logs` - Get system logs

---

## üé® Technology Stack

### AI & Inference
- **llama.cpp** - Native GGUF model inference
- **llama-cpp-python** - Python bindings for llama.cpp
- **CUDA** - GPU acceleration (optional)

### Frontend
- **Framework**: React 18 + TypeScript
- **Build Tool**: Vite
- **UI Library**: shadcn/ui (Radix UI + Tailwind CSS)
- **Routing**: React Router
- **State Management**: TanStack Query
- **WebSocket**: Native WebSocket API
- **Icons**: Lucide React

### Backend
- **Framework**: FastAPI
- **Server**: Uvicorn
- **WebSocket**: FastAPI WebSocket
- **Type Safety**: Pydantic
- **HTTP Client**: httpx (for downloads)

---

## üéØ Features Roadmap

### ‚úÖ Current (v1.0.1) - December 16, 2025
- ‚úÖ **llama.cpp Local Inference** - PRODUCTION READY!
- ‚úÖ GPU Acceleration (CUDA)
- ‚úÖ Chat with History Support
- ‚úÖ 4 GGUF Models Preconfigured
- ‚úÖ Model Download System (Ollama-style)
- ‚úÖ Live Progress Tracking
- ‚úÖ Multi-Registry Support
- ‚úÖ WebSocket Chat with Real AI
- ‚úÖ Basic UI with All Tabs

### Planned (v1.2.0) - Q1 2026
- üîÑ Voice Input (Whisper STT)
- üîÑ Voice Output (XTTS v2 TTS)
- üîÑ Model Switching Without Restart
- üîÑ Better Memory Integration
- üîÑ Performance Optimizations

### Future (v2.0.0) - Q2 2026
- üìã RAG (Retrieval-Augmented Generation)
- üìã Vector Database (ChromaDB/FAISS)
- üìã Multi-User Support
- üìã User Authentication
- üìã Cloud Deployment (AWS/GCP)
- üìã Mobile App
- üìã Advanced Plugin Marketplace

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## üìÑ License

**Apache License 2.0** with additional commercial restriction.

This project is licensed under the Apache License 2.0 with the following **additional restriction**:

> **Commercial use, sale, or redistribution of this software is prohibited without prior written permission from the copyright holder.**

This restriction applies only to the original J.A.R.V.I.S. source code and associated assets created by Lautloserspieler. All included third-party components (such as language models, speech libraries, or external APIs) remain under their respective licenses.

Full license: [LICENSE](./LICENSE)

---

## üôè Acknowledgments

- Inspired by JARVIS from Iron Man
- Built with [shadcn/ui](https://ui.shadcn.com/)
- Powered by [FastAPI](https://fastapi.tiangolo.com/)
- Local inference with [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Download system inspired by [Ollama](https://ollama.ai/)

---

## üìö Additional Documentation

- [LLM Download System](./docs/LLM_DOWNLOAD_SYSTEM.md) - Detailed download system documentation
- [Architecture](./docs/ARCHITECTURE.md) - System architecture overview
- [Implementation Status](./IMPLEMENTATION_STATUS.md) - Feature status and roadmap
- [Changelog](./docs/CHANGELOG.md) - Version history
- [Backend API](./backend/README.md) - Backend-specific documentation

---

<div align="center">

**Made with ‚ù§Ô∏è by the JARVIS Team**

*"Sometimes you gotta run before you can walk."* - Tony Stark

**Version:** 1.0.1 | **Last updated:** December 16, 2025, 11:15 CET

</div>
