# JARVIS Core Backend

## √úbersicht
FastAPI Backend f√ºr JARVIS Core System mit WebSocket-Support und REST-API-Endpunkten.

## Features
- ‚úÖ WebSocket f√ºr Echtzeitkommunikation
- ‚úÖ REST-API-Endpunkte f√ºr alle Services
- ‚úÖ CORS f√ºr Frontend-Verbindung aktiviert
- ‚úÖ Type-Safe mit Pydantic-Models
- ‚úÖ LLM Download-System (Ollama-Style)
- ‚úÖ SSE (Server-Sent Events) f√ºr Progress-Tracking
- ‚úÖ Model Management mit Multi-Registry-Support

## Installation

```bash
pip install -r requirements.txt
```

## Server starten

```bash
python main.py
```

Oder mit uvicorn:

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 5050
```

## API-Endpunkte

### Health
- `GET /` - Root-Endpunkt
- `GET /api/health` - Health-Check

### WebSocket
- `WS /ws` - WebSocket-Verbindung f√ºr Echtzeitkommunikation

### Chat
- `GET /api/chat/sessions` - Alle Chat-Sessions abrufen
- `POST /api/chat/sessions` - Neue Chat-Session erstellen
- `POST /api/chat/messages` - Nachricht senden

### Models
- `GET /api/models` - Alle Modelle abrufen
- `GET /api/models/available` - Verf√ºgbare Modelle mit Download-Status
- `GET /api/models/active` - Aktives Modell abrufen
- `POST /api/models/{id}/activate` - Modell aktivieren

#### Model Management (NEU in v1.0.1)
- `POST /api/models/download` - Model-Download starten
  ```json
  {
    "model_key": "mistral",
    "quantization": "Q4_K_M"
  }
  ```
- `GET /api/models/download/progress` - Download-Progress (SSE)
  - Server-Sent Events Stream
  - Liefert: `percent`, `speed_mbps`, `eta_seconds`, `status`
- `POST /api/models/cancel` - Download abbrechen
  ```json
  {
    "model_key": "mistral"
  }
  ```
- `GET /api/models/variants?model_key=mistral` - Quantization-Varianten abrufen
  - Liefert: Liste von URLs f√ºr Q4_K_M, Q5_K_M, Q6_K, Q8_0
- `DELETE /api/models/delete` - Modell l√∂schen
  ```json
  {
    "model_key": "mistral"
  }
  ```

### Plugins
- `GET /api/plugins` - Alle Plugins abrufen
- `POST /api/plugins/{id}/enable` - Plugin aktivieren
- `POST /api/plugins/{id}/disable` - Plugin deaktivieren

### Memory
- `GET /api/memory` - Erinnerungen abrufen
- `GET /api/memory/stats` - Memory-Statistiken abrufen
- `POST /api/memory/search` - Erinnerungen durchsuchen

### Logs
- `GET /api/logs` - Logs abrufen
- `GET /api/logs/stats` - Log-Statistiken abrufen

## WebSocket-Nachrichtenformat

### Client zu Server
```json
{
  "type": "chat_message",
  "message": "Hallo JARVIS",
  "sessionId": "session-id"
}
```

### Server zu Client
```json
{
  "type": "chat_response",
  "messageId": "uuid",
  "response": "Hallo! Wie kann ich helfen?",
  "sessionId": "session-id",
  "timestamp": "2025-12-16T10:00:00Z"
}
```

## Server-Sent Events (SSE)

### Download-Progress-Stream

**Endpunkt:** `GET /api/models/download/progress`

**Response-Format:**
```
event: progress
data: {"model":"mistral","percent":45.2,"speed_mbps":12.5,"eta_seconds":120,"status":"downloading"}

event: progress
data: {"model":"mistral","percent":100.0,"speed_mbps":0.0,"eta_seconds":0,"status":"completed"}

event: error
data: {"model":"mistral","error":"Download failed: Connection timeout"}
```

**Status-Werte:**
- `downloading` - Download l√§uft
- `completed` - Download abgeschlossen
- `cancelled` - Download abgebrochen
- `error` - Fehler aufgetreten

## LLM Download-System

### Multi-Registry-Support

Das Backend unterst√ºtzt mehrere Model-Registries:

1. **HuggingFace** (Standard)
   - `hf.co/namespace/repo`
   - `huggingface.co/user/model-GGUF`

2. **Ollama Registry**
   - `ollama.ai/library/llama2`

3. **Custom URLs**
   - `custom.ai/org/model`
   - Vollst√§ndige HTTPS-URLs

### Vordefinierte Modelle

| Shortcut | Registry | Namespace | Repository | Gr√∂√üe |
|----------|----------|-----------|------------|-------|
| `mistral` | HuggingFace | second-state | Mistral-Nemo-Instruct-2407-GGUF | ~4-8 GB |
| `qwen` | HuggingFace | bartowski | Qwen2.5-7B-Instruct-GGUF | ~4-8 GB |
| `deepseek` | HuggingFace | bartowski | DeepSeek-Coder-V2-Lite-Instruct-GGUF | ~4-7 GB |

### Quantization-Varianten

| Variant | Qualit√§t | Geschwindigkeit | Speicher | Use Case |
|---------|---------|----------------|----------|----------|
| **Q4_K_M** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ~4GB | **Standard** (balanced) |
| **Q4_K_S** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~3.5GB | Schnell, weniger RAM |
| **Q5_K_M** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ~5GB | H√∂here Qualit√§t |
| **Q6_K** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ~6GB | Sehr hohe Qualit√§t |
| **Q8_0** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ~8GB | Maximum Qualit√§t |

### Download-Features

- üîÑ **Resume-Support** - Unterbrochene Downloads werden fortgesetzt
- ‚úÖ **SHA256-Verifizierung** - Automatische Integrit√§tspr√ºfung
- üìä **Progress-Tracking** - Live-Updates via SSE
- üîê **HuggingFace Token** - Support f√ºr private Repositories
- üì¶ **Manifest-System** - Metadata-Management

## Environment Variables

Erstelle eine `.env` Datei:

```env
API_HOST=0.0.0.0
API_PORT=5050
CORS_ORIGINS=http://localhost:5000,http://localhost:5173
HUGGINGFACE_TOKEN=hf_YourTokenHere  # Optional f√ºr private Repos
```

## Projekt-Struktur

```
backend/
‚îú‚îÄ‚îÄ main.py                  # FastAPI Server
‚îú‚îÄ‚îÄ requirements.txt         # Python Dependencies
‚îî‚îÄ‚îÄ README.md               # Diese Datei

core/                       # Core-Module (Root-Level)
‚îú‚îÄ‚îÄ llm_manager.py          # LLM-Management
‚îú‚îÄ‚îÄ model_downloader.py     # Download-Engine
‚îú‚îÄ‚îÄ model_registry.py       # Multi-Registry
‚îú‚îÄ‚îÄ model_manifest.py       # Metadata-System
‚îî‚îÄ‚îÄ ...                     # Weitere Module
```

## Entwicklung

### Hot-Reload aktivieren

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 5050
```

### Logging-Level setzen

```bash
export LOG_LEVEL=DEBUG
python main.py
```

### API-Dokumentation

Nach dem Start verf√ºgbar unter:
- **Swagger UI**: http://localhost:5050/docs
- **ReDoc**: http://localhost:5050/redoc

## Troubleshooting

### Port 5050 bereits belegt

```bash
# Port in Umgebungsvariable √§ndern
export JARVIS_PORT=5051
python main.py
```

### Download schl√§gt fehl

```bash
# Debug-Modus aktivieren
export LOG_LEVEL=DEBUG
python main.py
```

### HuggingFace 403 Forbidden

```bash
# Token f√ºr private Repos setzen
export HUGGINGFACE_TOKEN="hf_YourTokenHere"
python main.py
```

## Dependencies

### Haupt-Dependencies
- `fastapi>=0.109.0` - Web-Framework
- `uvicorn>=0.27.0` - ASGI-Server
- `pydantic>=2.5.0` - Type-Safety
- `websockets>=12.0` - WebSocket-Support
- `httpx>=0.26.0` - HTTP-Client f√ºr Downloads

### Optional
- `transformers` - HuggingFace Models (geplant f√ºr v1.1)
- `torch` - PyTorch (geplant f√ºr v1.1)
- `llama-cpp-python` - llama.cpp Bindings (geplant f√ºr v1.1)

## Weitere Dokumentation

- [LLM Download-System](../docs/LLM_DOWNLOAD_SYSTEM.md)
- [Haupt-README](../README.md)
- [Changelog](../docs/CHANGELOG.md)

---

**Stand:** 16. Dezember 2025
