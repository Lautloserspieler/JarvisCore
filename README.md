# ğŸ¤– JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

Ein moderner KI-Assistent mit holographischer UI und **vollstÃ¤ndig lokaler llama.cpp Inferenz**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-cyan.svg)](https://golang.org)
[![Vue](https://img.shields.io/badge/Vue-3.5+-green.svg)](https://vuejs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

[ğŸ‡¬ğŸ‡§ English Version](./README_GB.md)

</div>

---

## âœ¨ Features

### ğŸ§  KI-Engine
- âœ… **llama.cpp Lokale Inferenz** - VollstÃ¤ndig implementiert und funktionsfÃ¤hig!
- âœ… **Automatische GPU-Erkennung** - NVIDIA CUDA Support
- âœ… **7 GGUF-Modelle** - Mistral, Qwen, DeepSeek, Llama und mehr
- âœ… **Chat mit History** - Kontext-bewusste Konversationen
- âœ… **Bis 32K Context** - Lange Konversationen mÃ¶glich
- âœ… **System-Prompts** - JARVIS-PersÃ¶nlichkeit konfigurierbar

### ğŸ¨ Frontend (Vue 3)
- âœ… **Holographische UI** - Beeindruckende JARVIS-inspirierte BenutzeroberflÃ¤che
- âœ… **Echtzeit-Chat** - WebSocket-basierte Live-Kommunikation
- âœ… **Sprach-Interface** - Voice-Input mit visueller RÃ¼ckmeldung
- âœ… **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Settings
- âœ… **Model-Management** - Download und Verwaltung von KI-Modellen
- âœ… **Plugin System** - Wetter, Timer, Notizen, News uvm.
- âœ… **Responsive Design** - Funktioniert auf allen BildschirmgrÃ¶ÃŸen
- âœ… **Dark Theme** - Cyberpunk-Ã„sthetik mit leuchtenden Effekten

### ğŸš€ Backend (Python + FastAPI)
- âœ… **FastAPI Server** - Hochperformanter Python Backend
- âœ… **llama.cpp Integration** - Native GGUF-Model-Inferenz
- âœ… **WebSocket Support** - Echtzeitkommunikation
- âœ… **RESTful API** - VollstÃ¤ndige REST-Endpunkte
- âœ… **Plugin System** - Erweiterbare Architektur
- âœ… **Memory Storage** - Konversationshistorie & Kontext

---

## ğŸ’» Voraussetzungen

- **Python 3.11+** - [python.org](https://python.org)
- **Node.js 18+** - [nodejs.org](https://nodejs.org)
- **Git** - [git-scm.com](https://git-scm.com)
- **(Optional)** NVIDIA GPU mit CUDA fÃ¼r beschleunigte Inferenz

---

## ğŸš€ Installation & Start

### Schritt 1: Repository klonen

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore
```

### Schritt 2: Basis-Dependencies installieren

```bash
pip install -r requirements.txt
```

### Schritt 3: llama.cpp Setup (ğŸ†• Automatisch!)

**NEU:** Automatische GPU-Erkennung und optimale Installation!

```bash
cd backend
python setup_llama.py
```

**Das Script erkennt automatisch:**
- âœ… NVIDIA GPU â†’ Installiert mit CUDA Support (30-50 tok/s)
- âœ… AMD GPU â†’ Empfiehlt CPU-Version (siehe unten)
- âœ… Keine GPU â†’ Installiert CPU-Version (5-10 tok/s)

**Ausgabe-Beispiel:**
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   JARVIS Core - llama.cpp Setup Script              â”‚
â”‚      Automatic GPU Detection & Install               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[INFO] System: Windows AMD64
[INFO] Python: 3.11.5
[INFO] Detecting GPU...
[INFO] NVIDIA GPU detected!

Installing llama-cpp-python with NVIDIA CUDA support

âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…

[SUCCESS] llama-cpp-python installed successfully!
[INFO] GPU Mode: NVIDIA CUDA
[INFO] You can now run: python main.py
```

### Schritt 4: Frontend Dependencies

```bash
cd ../frontend
npm install
cd ..
```

### Schritt 5: JARVIS starten

```bash
python main.py
```

**Das war's!** Das `main.py` Script:
- âœ… Startet automatisch Backend & Frontend
- âœ… Ã–ffnet Browser bei http://localhost:5000
- âœ… Backend lÃ¤uft auf http://localhost:5050

---

## ğŸ® Quick Start Alternative

### One-Liner Installation (Empfohlen)

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git && cd JarvisCore && pip install -r requirements.txt && cd backend && python setup_llama.py && cd ../frontend && npm install && cd .. && python main.py
```

---

## ğŸŒ Zugriffspunkte

Nach dem Start erreichst du:

- ğŸ¨ **Frontend UI**: http://localhost:5000
- ğŸ”§ **Backend API**: http://localhost:5050
- ğŸ“š **API-Dokumentation**: http://localhost:5050/docs
- ğŸ”Œ **WebSocket**: ws://localhost:5050/ws

---

## ğŸ§  llama.cpp Lokale Inferenz

**NEU in v1.1.0** - Production-ready mit automatischer GPU-Erkennung!

### Features
- ğŸš€ **GPU-Acceleration** - CUDA automatisch erkannt
- ğŸ¯ **GGUF-Support** - Alle llama.cpp-kompatiblen Modelle
- ğŸ’¬ **Chat-Modus** - History mit bis zu 32K Context
- âš¡ **Performance** - 30-50 tokens/sec (NVIDIA), 5-10 tokens/sec (CPU)

### GPU Support

| GPU-Typ | Support | Installation | Performance | Empfehlung |
|---------|---------|--------------|-------------|------------|
| **NVIDIA** | âœ… CUDA | Automatisch | âš¡âš¡âš¡ 30-50 tok/s | â­ Empfohlen |
| **AMD** | âš ï¸ ROCm | Komplex | âš¡âš¡âš¡ 25-40 tok/s | ğŸ‘‰ **Nutze CPU-Version** |
| **Intel Arc** | ğŸ”„ oneAPI | Coming Soon | âš¡âš¡ 20-35 tok/s | In Entwicklung |
| **CPU** | âœ… Standard | Automatisch | âš¡ 5-10 tok/s | âœ… Funktioniert |

#### ğŸ’¡ Hinweis fÃ¼r AMD GPU Nutzer:

**ROCm Setup ist komplex und erfordert:**
- Visual Studio Build Tools
- ROCm SDK Installation (~5 GB)
- Spezifische Treiber-Versionen
- Mehrere Neustarts
- Komplizierte Pfad-Konfiguration

**ğŸ‘‰ Empfehlung: Nutze die CPU-Version!**
```bash
python setup_llama.py
# WÃ¤hle Option 3: CPU-Version
```

**Vorteile CPU-Version:**
- âœ… Sofort einsatzbereit
- âœ… Keine komplexe Konfiguration
- âœ… Stabil und zuverlÃ¤ssig
- âœ… 5-10 tokens/sec (ausreichend fÃ¼r Chat)
- âœ… Kleinere Modelle (3B) laufen flÃ¼ssig

### VerfÃ¼gbare Modelle

| Model | GrÃ¶ÃŸe | Use Case | CPU Performance |
|-------|-------|----------|----------------|
| **Llama 3.2 3B** | ~2.0 GB | Klein, schnell | âš¡âš¡âš¡ 8-12 tok/s |
| **Phi-3 Mini** | ~2.3 GB | Kompakt, Chat | âš¡âš¡âš¡ 7-10 tok/s |
| **Qwen 2.5 7B** | ~5.2 GB | Vielseitig | âš¡âš¡ 5-8 tok/s |
| **Mistral 7B Nemo** | ~7.5 GB | Code, technisch | âš¡âš¡ 4-7 tok/s |
| **DeepSeek R1 8B** | ~6.9 GB | Analysen | âš¡ 3-6 tok/s |

**ğŸ‘‰ Empfehlung fÃ¼r CPU: Nutze Llama 3.2 3B oder Phi-3 Mini fÃ¼r beste Performance!**

---

## ğŸ”§ Manuelle llama.cpp Installation

Falls das automatische Script nicht funktioniert:

### NVIDIA GPU (CUDA)

```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

### CPU Only (Empfohlen fÃ¼r AMD)

```bash
cd backend
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### AMD GPU (ROCm) - Nur fÃ¼r Experten

âš ï¸ **Achtung:** Sehr komplex! Nur fÃ¼r erfahrene Nutzer empfohlen.

1. **ROCm installieren** (~5 GB): https://rocm.docs.amd.com/
2. **Visual Studio Build Tools** installieren
3. **Neustart erforderlich**
4. **Dann:**
```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_HIPBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

---

## ğŸ“¦ Model-Download-System

JARVIS Core nutzt ein **Ollama-inspiriertes Download-System**:

### Features
- ğŸ”„ **Multi-Registry-Support** - HuggingFace, Ollama, Custom URLs
- ğŸ“¦ **Resume-Downloads** - Unterbrochene Downloads fortsetzen
- âœ… **SHA256-Verifizierung** - Automatische IntegritÃ¤tsprÃ¼fung
- ğŸ“Š **Live-Progress** - Speed, ETA, Fortschrittsbalken
- ğŸ” **HuggingFace Token** - Support fÃ¼r private Repos

### Models verwalten

1. **JARVIS starten**: `python main.py`
2. **Web-UI Ã¶ffnen**: http://localhost:5000
3. **Models-Tab**: Navigation zur Model-Verwaltung
4. **Model downloaden**: Klick "Download" â†’ WÃ¤hle Quantization
5. **Model laden**: Klick "Load" bei heruntergeladenem Modell
6. **Chat starten**: Gehe zu "Chat" Tab und schreibe

Weitere Infos: [docs/LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## ğŸ”Œ Plugin System

**NEU in v1.1.0** - Erweiterbare Plugin-Architektur!

### VerfÃ¼gbare Plugins

| Plugin | Beschreibung | API-Key |
|--------|--------------|----------|
| â˜€ï¸ **Weather** | OpenWeatherMap Integration | âœ… Erforderlich |
| â° **Timer** | Timer & Erinnerungen | âŒ Nicht nÃ¶tig |
| ğŸ“ **Notes** | Schnelle Notizen | âŒ Nicht nÃ¶tig |
| ğŸ“° **News** | RSS News Feeds | âŒ Nicht nÃ¶tig |

### Plugin aktivieren

1. Ã–ffne **Plugins Tab** in der UI
2. Klicke **"Ğktivieren"** beim gewÃ¼nschten Plugin
3. Falls API-Key nÃ¶tig â†’ Modal Ã¶ffnet sich automatisch
4. Gib API-Key ein â†’ Wird sicher in `config/settings.json` gespeichert
5. Plugin ist aktiviert! âœ…

---

## ğŸ“ Projektstruktur

```
JarvisCore/
â”œâ”€â”€ main.py                 # ğŸš€ Unified Launcher
â”œâ”€â”€ requirements.txt        # ğŸ“¦ Python Dependencies
â”œâ”€â”€ core/                   # ğŸ§  Core Python Modules
â”‚   â”œâ”€â”€ llama_inference.py # llama.cpp Engine
â”‚   â”œâ”€â”€ model_downloader.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/                # ğŸ”§ Python/FastAPI Backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ setup_llama.py     # ğŸ†• Auto GPU Setup
â”‚   â”œâ”€â”€ plugin_manager.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/               # ğŸ¨ Vue 3 Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ plugins/                # ğŸ”Œ Plugin System
â”‚   â”œâ”€â”€ weather_plugin.py
â”‚   â”œâ”€â”€ timer_plugin.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/llm/             # ğŸ“¦ GGUF Models
â”œâ”€â”€ config/                 # âš™ï¸ Configuration
â”œâ”€â”€ data/                   # ğŸ—„ï¸ User Data
â”œâ”€â”€ docs/                   # ğŸ“š Documentation
â””â”€â”€ README.md
```

---

## ğŸ› Troubleshooting

### Problem: GPU nicht erkannt

```bash
# GPU-Status prÃ¼fen
nvidia-smi  # NVIDIA

# llama.cpp neu installieren
cd backend
python setup_llama.py
```

### Problem: Port bereits belegt

```bash
# Windows
netstat -ano | findstr :5000
netstat -ano | findstr :5050

# Linux/Mac
lsof -i :5000
lsof -i :5050
```

### Problem: Module nicht gefunden

```bash
pip install -r requirements.txt
cd frontend && npm install
```

### Problem: AMD GPU - ROCm Installation zu komplex

**LÃ¶sung: Nutze CPU-Version!**
```bash
cd backend
python setup_llama.py
# WÃ¤hle Option 3
```

Weitere Hilfe: [docs/TROUBLESHOOTING.md](./docs/TROUBLESHOOTING.md)

---

## ğŸ¯ Roadmap

### âœ… v1.1.0 (Current) - Dezember 2025
- âœ… Vue 3 Frontend
- âœ… Production-ready llama.cpp
- âœ… Automatische GPU-Erkennung
- âœ… Plugin System mit API-Key Management
- âœ… Model Download System

### ğŸ”„ v1.2.0 - Q1 2026
- Voice Input (Whisper)
- Voice Output (XTTS v2)
- Desktop App (Wails)
- Enhanced Memory System
- Docker Support

### ğŸ“‹ v2.0.0 - Q2 2026
- RAG Implementation
- Vector Database
- Multi-User Support
- Cloud Deployment
- Mobile App

---

## ğŸ¤ Contributing

BeitrÃ¤ge sind willkommen! Bitte lies [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

### Quick Start fÃ¼r Contributors

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Commit deine Ã„nderungen (`git commit -m 'feat: Add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. Erstelle einen Pull Request

---

## ğŸ“„ Lizenz

**Apache License 2.0** mit zusÃ¤tzlicher kommerzieller EinschrÃ¤nkung.

VollstÃ¤ndige Lizenz: [LICENSE](./LICENSE)

---

## ğŸ™ Danksagungen

- Inspiriert von JARVIS aus Iron Man
- Gebaut mit [Vue 3](https://vuejs.org/)
- Backend mit [FastAPI](https://fastapi.tiangolo.com/)
- Lokale Inferenz mit [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

## ğŸ“š Weitere Dokumentation

- [Quick Start Guide](docs/README_QUICKSTART.md)
- [Architecture Overview](docs/ARCHITECTURE.md)
- [LLM Download System](docs/LLM_DOWNLOAD_SYSTEM.md)
- [Performance Guide](docs/PERFORMANCE.md)
- [Deployment Guide](DEPLOYMENT.md)
- [Contributing Guidelines](CONTRIBUTING.md)
- [Changelog](docs/CHANGELOG.md)

---

<div align="center">

**Erstellt mit â¤ï¸ von Lautloserspieler**

*"Manchmal muss man rennen, bevor man gehen kann."* - Tony Stark

**Version:** 1.1.0 | **Release:** 02. Januar 2026

[â­ Star us on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [ğŸ› Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [ğŸ’¡ Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>
