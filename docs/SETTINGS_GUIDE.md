# JARVIS Settings Guide

## üß† Llama.cpp Inference Settings

### Max Tokens - Intelligent Response Completion

**Default: 2048 Tokens**

#### Wie funktioniert das?

JARVIS nutzt einen **intelligenten Stopp-Mechanismus**, der sicherstellt, dass Antworten **niemals mitten im Satz** abgeschnitten werden:

1. **EOS Token Detection** ‚úÖ  
   Das Modell sendet ein "End of Sequence" Token (`</s>`), wenn es seine Antwort beendet hat. JARVIS stoppt dann automatisch, **unabh√§ngig von max_tokens**.

2. **Smart Stopping** ‚úÖ  
   llama.cpp erkennt auch andere Stop-Sequenzen wie `<|user|>` und `<|system|>` und h√§lt rechtzeitig.

3. **max_tokens als Sicherheitsnetz** ‚ö°  
   max_tokens dient nur als **Obergrenze** f√ºr sehr lange Antworten. In 99% der F√§lle stoppt das Modell **vorher** nat√ºrlich durch EOS Token.

#### Empfohlene Werte:

| Modellgr√∂√üe | max_tokens | Warum? |
|--------------|------------|--------|
| 3B (Llama 3.2, Phi-3) | **2048** | Ausreichend f√ºr detaillierte Antworten |
| 7B (Qwen, Mistral) | **2048-4096** | Kann l√§ngere, komplexere Antworten geben |
| 8B+ (DeepSeek, Gemma) | **4096** | Maximal detaillierte Erkl√§rungen |

**‚ö†Ô∏è Wichtig:** Ein h√∂herer Wert f√ºr max_tokens bedeutet **NICHT** langsamere Antworten! Das Modell stoppt automatisch, wenn die Antwort fertig ist.

#### Was passiert bei zu niedrigen Werten?

**max_tokens = 512:** ‚ùå  
```
User: Erkl√§re mir neuronale Netzwerke
JARVIS: Ein neuronales Netzwerk ist ein Computermodell...
        [ABGESCHNITTEN MITTEN IM SATZ]
```

**max_tokens = 2048:** ‚úÖ  
```
User: Erkl√§re mir neuronale Netzwerke
JARVIS: Ein neuronales Netzwerk ist ein Computermodell, das...
        [VOLLST√ÑNDIGE ERKL√ÑRUNG]
        
        Zusammenfassend sind neuronale Netzwerke ein 
        m√§chtiges Werkzeug f√ºr KI-Anwendungen.
```

### Temperature (Kreativit√§t)

**Default: 0.7**

- **0.0 - 0.3:** Sehr deterministisch, pr√§zise, technisch  
  üëâ Gut f√ºr: Code, Fakten, Berechnungen

- **0.4 - 0.8:** Balanced, nat√ºrlich, leicht kreativ  
  üëâ Gut f√ºr: Chat, Erkl√§rungen, Konversation

- **0.9 - 1.5:** Sehr kreativ, variabel, manchmal √ºberraschend  
  üëâ Gut f√ºr: Storytelling, kreative Ideen

### Top-P (Nucleus Sampling)

**Default: 0.9**

Begrenzt die Token-Auswahl auf die wahrscheinlichsten Optionen:
- **0.9:** Empfohlen - gute Balance
- **0.95:** Etwas diverser
- **1.0:** Keine Begrenzung (kann zu random werden)

### Top-K

**Default: 40**

Begrenzt auf die Top-K wahrscheinlichsten Tokens:
- **40:** Standard-Wert, funktioniert gut
- **20:** Konservativer
- **80:** Diverser

### Repeat Penalty

**Default: 1.1**

Verhindert Wiederholungen:
- **1.0:** Keine Penalty
- **1.1:** Leichte Penalty (empfohlen)
- **1.3+:** Starke Penalty (kann unn√∂tig streng sein)

## üíæ Context Window

**Default: 8192 Tokens**

Die maximale Gr√∂√üe des "Ged√§chtnisses" f√ºr eine Konversation:
- System Prompt: ~200-300 Tokens
- Chat-Historie: Variable
- Neue Nachricht: Variable
- Antwort: bis max_tokens

**Wichtig:** Context Window muss gr√∂√üer sein als max_tokens!

## ‚è±Ô∏è Performance vs. Qualit√§t

### Schnelle Antworten (3B Modelle):
```json
{
  "max_tokens": 2048,
  "temperature": 0.7,
  "top_p": 0.9
}
```

### Beste Qualit√§t (7B+ Modelle):
```json
{
  "max_tokens": 4096,
  "temperature": 0.7,
  "top_p": 0.95
}
```

### Pr√§zise technische Antworten:
```json
{
  "max_tokens": 2048,
  "temperature": 0.3,
  "top_p": 0.9
}
```

## üîß Tipps & Tricks

1. **Antworten werden abgeschnitten?**  
   ‚Üí Erh√∂he max_tokens auf 2048 oder h√∂her

2. **Antworten zu repetitiv?**  
   ‚Üí Erh√∂he repeat_penalty auf 1.2

3. **Antworten zu "random"?**  
   ‚Üí Senke temperature auf 0.5-0.6

4. **Antworten zu "langweilig"?**  
   ‚Üí Erh√∂he temperature auf 0.8-0.9

5. **Model l√§uft zu langsam?**  
   ‚Üí Nutze kleineres Modell (3B statt 7B) oder aktiviere GPU

## üéØ Recommended Presets

### Default (Balanced)
```json
{
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 40,
  "repeat_penalty": 1.1,
  "max_tokens": 2048
}
```

### Technical Assistant
```json
{
  "temperature": 0.4,
  "top_p": 0.9,
  "top_k": 30,
  "repeat_penalty": 1.1,
  "max_tokens": 2048
}
```

### Creative Chatbot
```json
{
  "temperature": 0.9,
  "top_p": 0.95,
  "top_k": 50,
  "repeat_penalty": 1.15,
  "max_tokens": 3072
}
```

## ‚ùì FAQ

**Q: Warum stoppt JARVIS manchmal vor max_tokens?**  
A: Das ist **gewollt**! Das Modell sendet ein EOS Token wenn die Antwort fertig ist. max_tokens ist nur eine Obergrenze.

**Q: Kann ich max_tokens auf 10000 setzen?**  
A: Ja, aber achte darauf dass es kleiner als context_window ist. F√ºr die meisten Use-Cases reichen 2048-4096.

**Q: Macht ein h√∂herer max_tokens die Generierung langsamer?**  
A: **Nein!** Das Modell stoppt automatisch. Ein h√∂herer Wert ist nur eine Sicherheit.

**Q: Was ist besser: Hohe temperature oder hoher top_p?**  
A: **Beides zusammen!** Temperature kontrolliert Randomness, top_p begrenzt die Auswahl. Nutze beide.

---

**Made with ‚ù§Ô∏è for JARVIS Core v1.1.0**