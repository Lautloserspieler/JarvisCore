# ğŸ¤– JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

[![CI/CD](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/ci.yml/badge.svg)](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/ci.yml)
[![Release](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/release.yml/badge.svg)](https://github.com/Lautloserspieler/JarvisCore/actions/workflows/release.yml)
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-cyan.svg)](https://golang.org)
[![Vue](https://img.shields.io/badge/Vue-3.5+-green.svg)](https://vuejs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/Lautloserspieler/JarvisCore?style=social)](https://github.com/Lautloserspieler/JarvisCore)

Ein moderner KI-Assistent mit holographischer UI und **vollstÃ¤ndig lokaler llama.cpp Inferenz**

[ğŸ‡¬ğŸ‡§ English Version](./README_GB.md) | [ğŸ“š Docs](./docs/) | [â“ FAQ](./FAQ.md) | [ğŸ”’ Security](./SECURITY.md)

</div>

---

## ğŸš€ Quickstart

- **Schnellstart (manuell)**: [README_QUICKSTART.md](./README_QUICKSTART.md)
- **Fehlerbehebung**: [FAQ](./FAQ.md)

## âœ¨ Features

### ğŸ§  KI-Engine
- âœ… **llama.cpp Lokale Inferenz** - VollstÃ¤ndig implementiert und funktionsfÃ¤hig!
- âœ… **Automatische GPU-Erkennung** - NVIDIA CUDA Support
- âœ… **7 GGUF-Modelle** - Mistral, Qwen, DeepSeek, Llama und mehr
- âœ… **Chat mit History** - Kontext-bewusste Konversationen
- âœ… **Bis 32K Context** - Lange Konversationen mÃ¶glich
- âœ… **System-Prompts** - JARVIS-PersÃ¶nlichkeit konfigurierbar

### ğŸ™ï¸ Voice Control (v1.2.0 geplant)
> âš ï¸ **Hinweis:** Voice-Features (TTS/Whisper) sind aktuell **nicht Teil des Releases**. Die folgenden Punkte sind Roadmap/Entwicklung.
- ğŸ”„ **Voice Input** - Whisper-basierte Spracherkennung (in Entwicklung)
- ğŸ”„ **Voice Output** - XTTS v2 mit vorgeklonten JARVIS-Stimmen (in Entwicklung)
- âœ… **Vorgeklonte Voice-Samples** - Deutsch & Englisch (DE/EN v2.2)
- âœ… **Automatische Sprach-Erkennung** - Deutsch/Englisch Support
- âš¡ **Keine langwierige Berechnung** - Voice Samples vorkonfiguriert

### ğŸ¨ Frontend (Vue 3)
- âœ… **Holographische UI** - Beeindruckende JARVIS-inspirierte BenutzeroberflÃ¤che
- âœ… **Echtzeit-Chat** - WebSocket-basierte Live-Kommunikation
- âœ… **Sprach-Interface** - Voice-Input mit visueller RÃ¼ckmeldung
- âœ… **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Settings
- âœ… **Model-Management** - Download und Verwaltung von KI-Modellen
- âœ… **Plugin System** - Wetter, Timer, Notizen, News uvm.
- âœ… **Responsive Design** - Funktioniert auf allen BildschirmgrÃ¶ÃŸen
- âœ… **Dark Theme** - Cyberpunk-Ã„sthetik mit leuchtenden Effekten

### ğŸš€ Backend (Python + FastAPI)
- âœ… **FastAPI Server** - Hochperformanter Python Backend
- âœ… **llama.cpp Integration** - Native GGUF-Model-Inferenz
- âœ… **WebSocket Support** - Echtzeitkommunikation
- âœ… **RESTful API** - VollstÃ¤ndige REST-Endpunkte
- âœ… **Plugin System** - Erweiterbare Architektur
- âœ… **Memory Storage** - Konversationshistorie & Kontext

---

## ğŸ™ï¸ Voice Samples - Sofort einsatzbereit!

JarvisCore enthÃ¤lt **vorgeklonte JARVIS-Voice-Samples**, die keine langwierige Berechnung erfordern:
> âš ï¸ **Hinweis:** Die Voice-Pipeline selbst ist im aktuellen Release **noch nicht enthalten**.

### âœ¨ Vorteile der Vorgeklonten Stimmen

| Feature | Vorteil |
|---------|----------|
| âš¡ **Zeitersparnis** | 5-7 Minuten schneller beim ersten Start |
| ğŸ’» **Schwache PCs** | Funktioniert auch auf alten/schwachen Computern |
| ğŸ¯ **Sofort einsatzbereit** | Einfach klonen und starten - keine Wartezeit |
| ğŸŒ **MultilingualitÃ¤t** | Deutsch & Englisch Support (v2.2 optimiert) |
| ğŸ”Š **NatÃ¼rlicher Klang** | Hochwertig geclonte JARVIS-Stimmen |

### ğŸ“¦ Enthalten

- **`Jarvis_DE.wav`** - Deutsche JARVIS-Stimme (natÃ¼rlich, optimiert v2.2)
- **`Jarvis_EN.wav`** - Englische JARVIS-Stimme (natÃ¼rlich, optimiert v2.2)

**Speicherort:** `models/tts/voices/`

Siehe [models/tts/voices/README.md](./models/tts/voices/README.md) fÃ¼r technische Details.

---

## ğŸ’» Voraussetzungen

- **Python 3.11+** - [python.org](https://python.org)
- **Node.js 18+** - [nodejs.org](https://nodejs.org)
- **Git** - [git-scm.com](https://git-scm.com)
- **(Optional)** NVIDIA GPU mit CUDA fÃ¼r beschleunigte Inferenz

---

## ğŸš€ Installation & Start

### ğŸ“¦ Manuelle Installation (Empfohlen)

#### Option A: Neue Methode (v1.2.0-dev) - Empfohlen

```bash
# Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Installiere JarvisCore mit essentiellen Features
pip install -e ".[all]"

# Installiere Frontend Dependencies
cd frontend
npm install
cd ..

# Starte JARVIS (Web-Modus)
jarviscore web
```

Danach Ã¶ffnet sich automatisch: **http://localhost:5050**

#### Option B: Mit GPU Support (NVIDIA CUDA)

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Mit CUDA Support
pip install -e ".[tts,cuda]"

cd frontend && npm install && cd ..
jarviscore web
```

#### Option C: Development Setup (Mit Testing Tools)

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Mit allen Dev-Tools
pip install -e ".[dev,tts,cuda]"

cd frontend && npm install && cd ..

# Tests
pytest

# Start (Web-Modus)
jarviscore web
```

#### Option D: Alte Methode (v1.1.0 - Legacy, wird entfernt)

```bash
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

pip install -r requirements.txt  # Legacy
cd backend && python setup_llama.py && cd ..
cd frontend && npm install && cd ..
python scripts/start_web.py
```

> ğŸ’¡ **Tipp:** Neue Methoden sind kÃ¼rzer und Ã¼bersichtlicher!

---

## ğŸ”„ Start-Modi (Web/Desk/Prod)

**Welche Variante ist richtig?**

- **Web**: Backend + Frontend (Vite). Ideal fÃ¼r Entwicklung/Testing im Browser.
- **Desktop**: Backend + Wails Dev Mode. FÃ¼r UI-Entwicklung am Desktop.
- **Prod**: Backend + Desktop-Binary. FÃ¼r lokale Produktion/Demo ohne Dev-Tools.

**Kurzbeispiele:**

```bash
# Web UI im Browser
jarviscore web

# Desktop UI (Dev)
jarviscore desktop

# Desktop UI (Production Binary)
jarviscore prod
```

Alternativ kannst du die Skripte direkt nutzen:

```bash
python scripts/start_web.py
python scripts/start_desktop.py
python scripts/start_production.py
```

## ğŸ”„ CLI Commands (NEU in v1.2.0-dev)

```bash
# Web Mode (Development) - EMPFOHLEN
jarviscore web
# Ã–ffnet automatisch http://localhost:5050

# Desktop Mode (Wails Dev)
jarviscore desktop

# Production Mode (Desktop Binary)
jarviscore prod

# Hilfe anzeigen
jarviscore --help
```

---

## ğŸ“¦ Dependency Management (Neu in v1.2.0-dev)

### Old Way âŒ (Legacy, wird entfernt)
```bash
pip install -r requirements.txt
# Problem: Alle Dependencies, auch wenn nicht nÃ¶tig
# Hinweis: requirements*.txt sind Legacy und werden schrittweise entfernt.
```

### New Way âœ…
```bash
# WÃ¤hle genau, was du brauchst!
pip install -e "."              # Minimal
pip install -e ".[tts]"         # + Text-to-Speech
pip install -e ".[cuda]"        # + GPU Support (NVIDIA)
pip install -e ".[dev]"         # + Development Tools
pip install -e ".[ci]"          # + CI/CD Tools
pip install -e ".[all]"         # Alles zusammen

# Kombinationen mÃ¶glich
pip install -e ".[dev,tts,cuda]"
```

### VerfÃ¼gbare Extras

| Extra | Inhalt | GrÃ¶ÃŸe |
|-------|--------|-------|
| `tts` | XTTS v2 Voice Synthesis | ~500 MB |
| `cuda` | PyTorch with CUDA (NVIDIA) | ~2 GB |
| `dev` | Testing, Linting, Documentation | ~300 MB |
| `ci` | CI/CD Tools | ~100 MB |
| `all` | Alles zusammen | ~3 GB |

### ğŸ“š Mehr Infos

Siehe [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md) fÃ¼r vollstÃ¤ndige v1.1 â†’ v1.2 Migration

---

## ğŸŒ Zugriffspunkte

Nach dem Start erreichst du:

- ğŸ¨ **Frontend UI**: http://localhost:5050
- ğŸ”§ **Backend API**: http://localhost:5050
- ğŸ“š **API-Dokumentation**: http://localhost:5050/docs
- ğŸ”Œ **WebSocket**: ws://localhost:5050/ws

---

## ğŸ§  llama.cpp Lokale Inferenz

**NEU in v1.1.0** - Production-ready mit automatischer GPU-Erkennung!

### Features
- ğŸš€ **GPU-Acceleration** - CUDA automatisch erkannt
- ğŸ¯ **GGUF-Support** - Alle llama.cpp-kompatiblen Modelle
- ğŸ’¬ **Chat-Modus** - History mit bis zu 32K Context
- âš¡ **Performance** - 30-50 tokens/sec (NVIDIA), 5-10 tokens/sec (CPU)

### GPU Support

| GPU-Typ | Support | Installation | Performance | Empfehlung |
|---------|---------|--------------|-------------|------------|
| **NVIDIA** | âœ… CUDA | Automatisch | âš¡âš¡âš¡ 30-50 tok/s | â­ Empfohlen |
| **AMD** | âš ï¸ ROCm | Komplex | âš¡âš¡âš¡ 25-40 tok/s |In Entwicklung ğŸ‘‰ **Nutze CPU-Version** |
| **Intel Arc** | ğŸ”„ oneAPI | Coming Soon | âš¡âš¡ 20-35 tok/s | In Entwicklung |
| **CPU** | âœ… Standard | Automatisch | âš¡ 5-10 tok/s | âœ… Funktioniert |

#### ğŸ’¡ Hinweis fÃ¼r AMD GPU Nutzer:

**ROCm Setup ist komplex und erfordert:**
- Visual Studio Build Tools
- ROCm SDK Installation (~5 GB)
- Spezifische Treiber-Versionen
- Mehrere Neustarts
- Komplizierte Pfad-Konfiguration

**ğŸ‘‰ Empfehlung: Nutze die CPU-Version!**
```bash
cd backend && python setup_llama.py
# WÃ¤hle Option 3: CPU-Version
```

**Vorteile CPU-Version:**
- âœ… Sofort einsatzbereit
- âœ… Keine komplexe Konfiguration
- âœ… Stabil und zuverlÃ¤ssig
- âœ… 5-10 tokens/sec (ausreichend fÃ¼r Chat)
- âœ… Kleinere Modelle (3B) laufen flÃ¼ssig

### VerfÃ¼gbare Modelle

| Model | GrÃ¶ÃŸe | Use Case | CPU Performance |
|-------|-------|----------|----------------|
| **Llama 3.2 3B** | ~2.0 GB | Klein, schnell | âš¡âš¡âš¡ 8-12 tok/s |
| **Phi-3 Mini** | ~2.3 GB | Kompakt, Chat | âš¡âš¡âš¡ 7-10 tok/s |
| **Qwen 2.5 7B** | ~5.2 GB | Vielseitig | âš¡âš¡ 5-8 tok/s |
| **Mistral 7B Nemo** | ~7.5 GB | Code, technisch | âš¡âš¡ 4-7 tok/s |
| **DeepSeek R1 8B** | ~6.9 GB | Analysen | âš¡ 3-6 tok/s |

**ğŸ‘‰ Empfehlung fÃ¼r CPU: Nutze Llama 3.2 3B oder Phi-3 Mini fÃ¼r beste Performance!**

---

## ğŸ”§ Manuelle llama.cpp Installation

Falls das automatische Script nicht funktioniert:

### NVIDIA GPU (CUDA)

```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

### CPU Only (Empfohlen fÃ¼r AMD)

```bash
cd backend
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### AMD GPU (ROCm) - Nur fÃ¼r Experten

âš ï¸ **Achtung:** Sehr komplex! Nur fÃ¼r erfahrene Nutzer empfohlen.

1. **ROCm installieren** (~5 GB): https://rocm.docs.amd.com/
2. **Visual Studio Build Tools** installieren
3. **Neustart erforderlich**
4. **Dann:**
```bash
cd backend
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_HIPBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python
```

---

## ğŸ“¦ Model-Download-System

JARVIS Core nutzt ein **Ollama-inspiriertes Download-System**:

### Features
- ğŸ”„ **Multi-Registry-Support** - HuggingFace, Ollama, Custom URLs
- ğŸ“¦ **Resume-Downloads** - Unterbrochene Downloads fortsetzen
- âœ… **SHA256-Verifizierung** - Automatische IntegritÃ¤tsprÃ¼fung
- ğŸ“Š **Live-Progress** - Speed, ETA, Fortschrittsbalken
- ğŸ” **HuggingFace Token** - Support fÃ¼r private Repos

### Models verwalten

1. **JARVIS starten**: `jarviscore web` oder `python scripts/start_web.py`
2. **Web-UI Ã¶ffnen**: http://localhost:5050
3. **Models-Tab**: Navigation zur Model-Verwaltung
4. **Model downloaden**: Klick "Download" â†’ WÃ¤hle Quantization
5. **Model laden**: Klick "Load" bei heruntergeladenem Modell
6. **Chat starten**: Gehe zu "Chat" Tab und schreibe

Weitere Infos: [docs/LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## ğŸ”Œ Plugin System

**NEU in v1.1.0** - Erweiterbare Plugin-Architektur!

### VerfÃ¼gbare Plugins

| Plugin | Beschreibung | API-Key |
|--------|--------------|----------|
| â˜€ï¸ **Weather** | OpenWeatherMap Integration | âœ… Erforderlich |
| â° **Timer** | Timer & Erinnerungen | âŒ Nicht nÃ¶tig |
| ğŸ“ **Notes** | Schnelle Notizen | âŒ Nicht nÃ¶tig |
| ğŸ“° **News** | RSS News Feeds | âŒ Nicht nÃ¶tig |

### Plugin aktivieren

1. Ã–ffne **Plugins Tab** in der UI
2. Klicke **"Aktivieren"** beim gewÃ¼nschten Plugin
3. Falls API-Key nÃ¶tig â†’ Modal Ã¶ffnet sich automatisch
4. Gib API-Key ein â†’ Wird sicher in `config/settings.json` gespeichert
5. Plugin ist aktiviert! âœ…

---

## ğŸ“ Projektstruktur

```
JarvisCore/
â”œâ”€â”€ pyproject.toml          # Centralized Configuration
â”œâ”€â”€ main.py                 # Unified Launcher
â”œâ”€â”€ requirements.txt        # Legacy (deprecated, wird entfernt)
â”œâ”€â”€ jarviscore/             # CLI Package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ cli.py
â”œâ”€â”€ scripts/                # Launcher Scripts
â”‚   â”œâ”€â”€ start_web.py
â”‚   â”œâ”€â”€ start_desktop.py
â”‚   â”œâ”€â”€ start_production.py
â”‚   â””â”€â”€ start_jarvis.py     # Legacy Wrapper
â”œâ”€â”€ core/                   # Core Python Modules
â”‚   â”œâ”€â”€ llama_inference.py # llama.cpp Engine
â”‚   â”œâ”€â”€ model_downloader.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/                # Python/FastAPI Backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ setup_llama.py     # Auto GPU Setup
â”‚   â”œâ”€â”€ plugin_manager.py
â”‚   â””â”€â”€ requirements*.txt  # Legacy (deprecated, wird entfernt)
â”œâ”€â”€ frontend/               # Vue 3 Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ plugins/                # Plugin System
â”‚   â”œâ”€â”€ weather_plugin.py
â”‚   â”œâ”€â”€ timer_plugin.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                 # Models
â”‚   â”œâ”€â”€ llm/               # GGUF LLM Models
â”‚   â””â”€â”€ tts/               # Voice Samples
â”‚       â””â”€â”€ voices/        # Pre-cloned JARVIS voices
â”œâ”€â”€ config/                 # Configuration
â”œâ”€â”€ data/                   # User Data
â”œâ”€â”€ docs/                   # Documentation
â””â”€â”€ README.md
```

---

## ğŸ› Troubleshooting

### Problem: GPU nicht erkannt

```bash
# GPU-Status prÃ¼fen
nvidia-smi  # NVIDIA

# llama.cpp neu installieren
cd backend
python setup_llama.py
```

### Problem: Port bereits belegt

```bash
# Windows
netstat -ano | findstr :5050

# Linux/Mac
lsof -i :5050
```

### Problem: Module nicht gefunden

```bash
# Neue Methode
pip install -e ".[tts]"

# Oder alte Methode (Legacy)
pip install -r requirements.txt
cd frontend && npm install
```

### Problem: AMD GPU - ROCm Installation zu komplex

**LÃ¶sung: Nutze CPU-Version!**
```bash
cd backend
python setup_llama.py
# WÃ¤hle Option 3
```

Weitere Hilfe: [â“ FAQ](./FAQ.md) | [ğŸ“š Troubleshooting](./docs/TROUBLESHOOTING.md) | [ğŸ“‹ Migration Guide](./MIGRATION_GUIDE.md)

---

## âš™ï¸ Konfiguration (.env)

Die vollstÃ¤ndige Liste aller Umgebungsvariablen findest du in der Vorlage: [`.env.example`](./.env.example).
Eine kurze Einordnung, welche Variablen wofÃ¼r gedacht sind (LLM, TTS, Plugins, Feature-Flags), gibt es hier:
[ğŸ“˜ Konfiguration & Env-Variablen](./docs/CONFIGURATION.md).

---

## ğŸ¯ Roadmap

### âœ… v1.1.0 (Current) - Dezember 2025
- âœ… Vue 3 Frontend
- âœ… Production-ready llama.cpp
- âœ… Automatische GPU-Erkennung
- âœ… Plugin System mit API-Key Management
- âœ… Model Download System
- âœ… Vorgeklonte Voice Samples (DE/EN v2.2)

### ğŸ”„ v1.2.0 (Q1 2026) - NEW!
- âœ… **Consolidated Dependency Management** (pyproject.toml)
- âœ… **CLI Entry Points** (jarviscore web/desktop/prod)
- âœ… **Enhanced Configuration** (50+ settings in .env.example)
- âœ… **GPU Selection** (NVIDIA CUDA / AMD ROCm / CPU)
- ğŸ”„ Voice Input (Whisper)
- ğŸ”„ Voice Output (XTTS v2)
- ğŸ”„ Desktop App (Wails)
- ğŸ”„ Enhanced Memory System
- ğŸ”„ Docker Support

### ğŸ“‹ v2.0.0 - Q2 2026
- RAG Implementation
- Vector Database
- Multi-User Support
- Cloud Deployment

Siehe auch: [ğŸ“‹ CHANGELOG](./CHANGELOG.md) fÃ¼r detaillierte Release Notes

---

## ğŸ¤ Contributing

BeitrÃ¤ge sind willkommen! Bitte lies [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

### Quick Start fÃ¼r Contributors

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Commit deine Ã„nderungen (`git commit -m 'feat: Add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. Erstelle einen Pull Request

**Neuer Development Setup:**
```bash
# Installation mit allen Dev-Tools
pip install -e ".[dev,ci,tts,cuda]"

# Tests ausfÃ¼hren
pytest

# Code formatieren
black .
ruff check .
```

---

## ğŸ“„ Lizenz

**Apache License 2.0** mit zusÃ¤tzlicher kommerzieller EinschrÃ¤nkung.

VollstÃ¤ndige Lizenz: [LICENSE](./LICENSE)

---

## ğŸ™ Danksagungen

- Inspiriert von JARVIS aus Iron Man
- Gebaut mit [Vue 3](https://vuejs.org/)
- Backend mit [FastAPI](https://fastapi.tiangolo.com/)
- Lokale Inferenz mit [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

## ğŸ“š Weitere Dokumentation

- [ğŸ® GPU Selection Guide](./docs/GPU_SELECTION.md) - NEW!
- [âš™ï¸ Konfiguration (.env)](./docs/CONFIGURATION.md)
- [ğŸ“‹ Migration Guide v1.1 â†’ v1.2](MIGRATION_GUIDE.md)
- [ğŸ—ï¸ Architecture Refactor Plan](ARCHITECTURE_REFACTOR.md)
- [Quick Start Guide](docs/README_QUICKSTART.md)
- [Architecture Overview](docs/ARCHITECTURE.md)
- [LLM Download System](docs/LLM_DOWNLOAD_SYSTEM.md)
- [Performance Guide](docs/PERFORMANCE.md)
- [Voice Samples Guide](models/tts/voices/README.md)
- [Deployment Guide](DEPLOYMENT.md)
- [Contributing Guidelines](CONTRIBUTING.md)
- [FAQ](FAQ.md)
- [Changelog](CHANGELOG.md)

---

<div align="center">

**Erstellt mit â¤ï¸ von Lautloserspieler**

*"Manchmal muss man rennen, bevor man gehen kann."* - Tony Stark

**Version:** 1.1.0 | **v1.2.0-dev Phase 1 âœ…** | **Release:** 02. Januar 2026

[â­ Star us on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [ğŸ› Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [ğŸ’¡ Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>
