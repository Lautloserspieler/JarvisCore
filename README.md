# ü§ñ JARVIS Core System

<div align="center">

**Just A Rather Very Intelligent System**

Ein moderner KI-Assistent mit holographischer UI und **vollst√§ndig lokaler llama.cpp Inferenz**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Go](https://img.shields.io/badge/Go-1.21+-cyan.svg)](https://golang.org)
[![Vue](https://img.shields.io/badge/Vue-3.5+-green.svg)](https://vuejs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-GGUF-orange.svg)](https://github.com/ggerganov/llama.cpp)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

[üá¨üáß English Version](./README_GB.md)

</div>

---

## ‚ú® Features

### üß† KI-Engine
- ‚úÖ **llama.cpp Lokale Inferenz** - Vollst√§ndig implementiert und funktionsf√§hig!
- ‚úÖ **GPU-Acceleration** - Automatische CUDA-Erkennung (30-50 tok/s)
- ‚úÖ **4 GGUF-Modelle** - Mistral, Qwen, DeepSeek, Llama 2 (Q4_K_M)
- ‚úÖ **Chat mit History** - Kontext-bewusste Konversationen
- ‚úÖ **Bis 32K Context** - Lange Konversationen m√∂glich
- ‚úÖ **System-Prompts** - JARVIS-Pers√∂nlichkeit konfigurierbar

### üé® Frontend (Vue 3)
- ‚úÖ **Holographische UI** - Beeindruckende JARVIS-inspirierte Benutzeroberfl√§che
- ‚úÖ **Echtzeit-Chat** - WebSocket-basierte Live-Kommunikation
- ‚úÖ **Sprach-Interface** - Voice-Input mit visueller R√ºckmeldung
- ‚úÖ **Multi-Tab Navigation** - Chat, Dashboard, Memory, Models, Settings
- ‚úÖ **Model-Management** - Download und Verwaltung von KI-Modellen
- ‚úÖ **Responsive Design** - Funktioniert auf allen Bildschirmgr√∂√üen
- ‚úÖ **Dark Theme** - Cyberpunk-√Ñsthetik mit leuchtenden Effekten

### üöÄ Backend (Python + FastAPI)
- ‚úÖ **FastAPI Server** - Hochperformanter Python Backend
- ‚úÖ **llama.cpp Integration** - Native GGUF-Model-Inferenz
- ‚úÖ **WebSocket Support** - Echtzeitkommunikation
- ‚úÖ **RESTful API** - Vollst√§ndige REST-Endpunkte
- ‚úÖ **Plugin System** - Erweiterbare Architektur
- ‚úÖ **Memory Storage** - Konversationshistorie & Kontext

---

## üíª Voraussetzungen

- **Python 3.11+** - [python.org](https://python.org)
- **Node.js 18+** - [nodejs.org](https://nodejs.org)
- **Git** - [git-scm.com](https://git-scm.com)
- **(Optional)** NVIDIA GPU mit CUDA f√ºr beschleunigte Inferenz

---

## üöÄ Installation & Start

### Windows

```powershell
# Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Python Dependencies installieren
pip install -r requirements.txt

# Frontend Dependencies installieren
cd frontend
npm install
cd ..

# JARVIS starten
python main.py
```

### Linux / macOS

```bash
# Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# Python Dependencies installieren
pip install -r requirements.txt

# Frontend Dependencies installieren
cd frontend
npm install
cd ..

# JARVIS starten
python main.py
```

**Das war's!** Das `main.py` Script:
- ‚úÖ Startet automatisch Backend & Frontend
- ‚úÖ √ñffnet Browser bei http://localhost:5000
- ‚úÖ Backend l√§uft auf http://localhost:5050

---

## üåê Zugriffspunkte

Nach dem Start erreichst du:

- üé® **Frontend UI**: http://localhost:5000
- üîß **Backend API**: http://localhost:5050
- üìö **API-Dokumentation**: http://localhost:5050/docs
- üîå **WebSocket**: ws://localhost:5050/ws

---

## üß† llama.cpp Lokale Inferenz

**NEU in v1.1.0** - Production-ready!

### Features
- üöÄ **GPU-Acceleration** - CUDA automatisch erkannt
- üéØ **GGUF-Support** - Alle llama.cpp-kompatiblen Modelle
- üí¨ **Chat-Modus** - History mit bis zu 32K Context
- ‚ö° **Performance** - 30-50 tokens/sec (GPU), 5-10 tokens/sec (CPU)

### Verf√ºgbare Modelle

| Model | Gr√∂√üe | Use Case | Performance |
|-------|-------|----------|-------------|
| **Mistral 7B Nemo** | ~7.5 GB | Code, technische Details | ‚ö°‚ö°‚ö° |
| **Qwen 2.5 7B** | ~5.2 GB | Vielseitig, multilingual | ‚ö°‚ö°‚ö° |
| **DeepSeek R1 8B** | ~6.9 GB | Analysen, Reasoning | ‚ö°‚ö° |
| **Llama 2 7B** | ~4.0 GB | Kreativ, Chat | ‚ö°‚ö°‚ö° |

### Verwendung

```python
from core.llama_inference import llama_runtime

# Modell laden
llama_runtime.load_model(
    model_path="models/llm/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    model_name="mistral",
    n_ctx=8192,
    n_gpu_layers=-1
)

# Chat mit History
result = llama_runtime.chat(
    message="Erkl√§re mir Quantencomputing",
    history=[...],
    system_prompt="Du bist JARVIS...",
    temperature=0.7
)
```

---

## üì¶ Model-Download-System

JARVIS Core nutzt ein **Ollama-inspiriertes Download-System**:

### Features
- üîÑ **Multi-Registry-Support** - HuggingFace, Ollama, Custom URLs
- üì¶ **Resume-Downloads** - Unterbrochene Downloads fortsetzen
- ‚úÖ **SHA256-Verifizierung** - Automatische Integrit√§tspr√ºfung
- üìä **Live-Progress** - Speed, ETA, Fortschrittsbalken
- üîê **HuggingFace Token** - Support f√ºr private Repos

### Models verwalten

1. **JARVIS starten**: `python main.py`
2. **Web-UI √∂ffnen**: http://localhost:5000
3. **Models-Tab**: Navigation zur Model-Verwaltung
4. **Model downloaden**: Klick "Download" ‚Üí W√§hle Quantization
5. **Model laden**: Klick "Load" bei heruntergeladenem Modell
6. **Chat starten**: Gehe zu "Chat" Tab und schreibe

Weitere Infos: [docs/LLM_DOWNLOAD_SYSTEM.md](./docs/LLM_DOWNLOAD_SYSTEM.md)

---

## üìÅ Projektstruktur

```
JarvisCore/
‚îú‚îÄ‚îÄ main.py                 # üöÄ Unified Launcher
‚îú‚îÄ‚îÄ requirements.txt        # üì¶ Python Dependencies
‚îú‚îÄ‚îÄ core/                   # üß† Core Python Modules
‚îÇ   ‚îú‚îÄ‚îÄ llama_inference.py # llama.cpp Engine
‚îÇ   ‚îú‚îÄ‚îÄ model_downloader.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ backend/                # üîß Python/FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ frontend/               # üé® Vue 3 Frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îú‚îÄ‚îÄ models/llm/             # üì¶ GGUF Models
‚îú‚îÄ‚îÄ config/                 # ‚öôÔ∏è Configuration
‚îú‚îÄ‚îÄ data/                   # üóÑÔ∏è User Data
‚îú‚îÄ‚îÄ docs/                   # üìö Documentation
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Development

### Backend separat starten

```bash
cd backend
pip install -r requirements.txt
python main.py
# L√§uft auf http://localhost:5050
```

### Frontend separat starten

```bash
cd frontend
npm install
npm run dev
# L√§uft auf http://localhost:5173
```

### Tests ausf√ºhren

```bash
# Backend-Tests
cd backend
pytest tests/ -v

# Frontend-Tests
cd frontend
npm run test
```

---

## üîå API-Endpunkte

### Chat
- `WS /ws` - WebSocket-Chat mit AI
- `GET /api/chat/sessions` - Chat-Sessions
- `POST /api/chat/sessions` - Neue Session

### Models
- `GET /api/models` - Alle Modelle
- `POST /api/models/{id}/load` - Modell laden
- `POST /api/models/download` - Download starten
- `DELETE /api/models/delete` - Modell l√∂schen

### System
- `GET /api/health` - Health-Check
- `GET /api/logs` - System-Logs

Vollst√§ndige API-Docs: http://localhost:5050/docs

---

## üé® Technologie-Stack

### Frontend
- **Framework**: Vue 3 + TypeScript
- **Build**: Vite
- **UI**: Tailwind CSS + Custom Components
- **State**: Pinia
- **WebSocket**: Native API

### Backend
- **Python**: FastAPI + Uvicorn
- **AI**: llama.cpp + CUDA
- **WebSocket**: FastAPI WebSocket
- **Storage**: Local File System

---

## üêõ Troubleshooting

### Problem: Port bereits belegt

```bash
# Windows
netstat -ano | findstr :5000
netstat -ano | findstr :5050

# Linux/Mac
lsof -i :5000
lsof -i :5050

# Prozess beenden und neu starten
```

### Problem: Module nicht gefunden

```bash
# Alle Dependencies neu installieren
pip install -r requirements.txt
cd frontend && npm install
```

### Problem: CUDA nicht erkannt

```bash
# CUDA-Installation pr√ºfen
nvidia-smi

# Python CUDA-Bindings installieren
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

Weitere Hilfe: [docs/TROUBLESHOOTING.md](./docs/TROUBLESHOOTING.md)

---

## üéØ Roadmap

### ‚úÖ v1.1.0 (Current) - Dezember 2025
- ‚úÖ Vue 3 Frontend
- ‚úÖ Production-ready llama.cpp
- ‚úÖ Community Documentation
- ‚úÖ Model Download System

### üîÑ v1.2.0 - Q1 2026
- Voice Input (Whisper)
- Voice Output (XTTS v2)
- Desktop App (Wails)
- Enhanced Memory System
- Docker Support

### üìã v2.0.0 - Q2 2026
- RAG Implementation
- Vector Database
- Multi-User Support
- Cloud Deployment
- Mobile App

---

## ü§ù Contributing

Beitr√§ge sind willkommen! Bitte lies [CONTRIBUTING.md](CONTRIBUTING.md) f√ºr Details.

### Quick Start f√ºr Contributors

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/amazing-feature`)
3. Commit deine √Ñnderungen (`git commit -m 'feat: Add amazing feature'`)
4. Push zum Branch (`git push origin feature/amazing-feature`)
5. Erstelle einen Pull Request

Bitte beachte:
- [Code of Conduct](CODE_OF_CONDUCT.md)
- [Security Policy](SECURITY.md)
- [Coding Standards](CONTRIBUTING.md#coding-standards)

---

## üìÑ Lizenz

**Apache License 2.0** mit zus√§tzlicher kommerzieller Einschr√§nkung.

Dieses Projekt ist unter der Apache License 2.0 lizenziert mit folgender **zus√§tzlicher Einschr√§nkung**:

> **Kommerzielle Nutzung, Verkauf oder Weitervertrieb dieser Software ist ohne vorherige schriftliche Genehmigung des Copyright-Inhabers untersagt.**

Vollst√§ndige Lizenz: [LICENSE](./LICENSE)

---

## üîí Security

Sicherheitsl√ºcken bitte **nicht** als GitHub Issue melden. Nutze stattdessen:
- GitHub Security Advisory
- Email (siehe [SECURITY.md](SECURITY.md))

Weitere Infos: [SECURITY.md](SECURITY.md)

---

## üôè Danksagungen

- Inspiriert von JARVIS aus Iron Man
- Gebaut mit [Vue 3](https://vuejs.org/)
- Backend mit [FastAPI](https://fastapi.tiangolo.com/)
- Lokale Inferenz mit [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Containerisierung mit [Docker](https://docker.com) (coming in v1.2)

---

## üìö Weitere Dokumentation

- [Quick Start Guide](docs/README_QUICKSTART.md)
- [Architecture Overview](docs/ARCHITECTURE.md)
- [LLM Download System](docs/LLM_DOWNLOAD_SYSTEM.md)
- [Performance Guide](docs/PERFORMANCE.md)
- [Deployment Guide](DEPLOYMENT.md)
- [Contributing Guidelines](CONTRIBUTING.md)
- [Changelog](docs/CHANGELOG.md)

---

<div align="center">

**Erstellt mit ‚ù§Ô∏è von Lautloserspieler**

*"Manchmal muss man rennen, bevor man gehen kann."* - Tony Stark

**Version:** 1.1.0 | **Release:** 02. Januar 2026

[‚≠ê Star us on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [üêõ Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [üí° Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>