# â“ Frequently Asked Questions (FAQ)

**Willkommen bei JARVIS Core!** Hier findest du Antworten auf die hÃ¤ufigsten Fragen.

[ğŸ‡¬ğŸ‡§ English FAQ](./FAQ_EN.md) | [ğŸ› Issues](https://github.com/Lautloserspieler/JarvisCore/issues) | [ğŸ“š Docs](./docs/)

---

## ğŸ“š Inhaltsverzeichnis

### [Allgemein](#allgemein-1)
- [Was ist JARVIS Core?](#was-ist-jarvis-core)
- [Ist JARVIS wirklich 100% lokal?](#ist-jarvis-wirklich-100-lokal)
- [Kostet JARVIS etwas?](#kostet-jarvis-etwas)
- [Welche Betriebssysteme werden unterstÃ¼tzt?](#welche-betriebssysteme-werden-unterstÃ¼tzt)

### [Installation & Setup](#installation--setup-1)
- [Wie installiere ich JARVIS?](#wie-installiere-ich-jarvis)
- [Brauche ich eine GPU?](#brauche-ich-eine-gpu)
- [Welche GPU wird unterstÃ¼tzt?](#welche-gpu-wird-unterstÃ¼tzt)
- [Wie viel RAM brauche ich?](#wie-viel-ram-brauche-ich)
- [Wie viel Speicherplatz wird benÃ¶tigt?](#wie-viel-speicherplatz-wird-benÃ¶tigt)

### [Modelle](#modelle-1)
- [Welche AI-Modelle kann ich nutzen?](#welche-ai-modelle-kann-ich-nutzen)
- [Wie downloade ich ein Modell?](#wie-downloade-ich-ein-modell)
- [Welches Modell ist am schnellsten?](#welches-modell-ist-am-schnellsten)
- [Kann ich eigene Modelle nutzen?](#kann-ich-eigene-modelle-nutzen)
- [Wie groÃŸ sind die Modelle?](#wie-groÃŸ-sind-die-modelle)

### [Performance](#performance-1)
- [Wie schnell ist JARVIS?](#wie-schnell-ist-jarvis)
- [Warum ist mein JARVIS langsam?](#warum-ist-mein-jarvis-langsam)
- [Kann ich die Performance verbessern?](#kann-ich-die-performance-verbessern)
- [AMD GPU - Warum so langsam?](#amd-gpu---warum-so-langsam)

### [Features & Nutzung](#features--nutzung-1)
- [Kann JARVIS Bilder generieren?](#kann-jarvis-bilder-generieren)
- [Funktioniert Spracherkennung?](#funktioniert-spracherkennung)
- [Kann JARVIS im Internet suchen?](#kann-jarvis-im-internet-suchen)
- [Wie aktiviere ich Plugins?](#wie-aktiviere-ich-plugins)
- [Kann ich meinen Chat-Verlauf speichern?](#kann-ich-meinen-chat-verlauf-speichern)

### [Troubleshooting](#troubleshooting-1)
- [JARVIS startet nicht - was tun?](#jarvis-startet-nicht---was-tun)
- [Port bereits belegt - wie beheben?](#port-bereits-belegt---wie-beheben)
- [Module not found Error](#module-not-found-error)
- [GPU wird nicht erkannt](#gpu-wird-nicht-erkannt)
- [Frontend lÃ¤dt nicht](#frontend-lÃ¤dt-nicht)

### [Sicherheit & PrivatsphÃ¤re](#sicherheit--privatsphÃ¤re-1)
- [Werden meine Daten gesammelt?](#werden-meine-daten-gesammelt)
- [Ist JARVIS sicher?](#ist-jarvis-sicher)
- [Wo werden Chat-Daten gespeichert?](#wo-werden-chat-daten-gespeichert)
- [Kann ich Telemetrie deaktivieren?](#kann-ich-telemetrie-deaktivieren)

### [Entwicklung & Community](#entwicklung--community-1)
- [Kann ich zu JARVIS beitragen?](#kann-ich-zu-jarvis-beitragen)
- [Wie melde ich Bugs?](#wie-melde-ich-bugs)
- [Gibt es einen Discord/Community?](#gibt-es-einen-discordcommunity)
- [Roadmap - Was kommt als nÃ¤chstes?](#roadmap---was-kommt-als-nÃ¤chstes)

---

## Allgemein

### Was ist JARVIS Core?

**JARVIS Core** ist ein **lokaler AI-Assistent**, inspiriert von Tony Starks JARVIS aus Iron Man. Anders als ChatGPT, Claude oder Gemini lÃ¤uft JARVIS **komplett auf deinem Computer** - ohne Cloud, ohne Datenweitergabe, mit voller Kontrolle.

**Key Features:**
- ğŸ”’ **100% Lokal** - Keine Internetverbindung nach der Installation
- ğŸ†“ **Kostenlos** - Open-Source, keine Abos, keine versteckten Kosten
- âš¡ **Schnell** - GPU-beschleunigt (NVIDIA CUDA)
- ğŸ¨ **Modern** - Holographische UI inspiriert von Iron Man
- ğŸ”Œ **Erweiterbar** - Plugin-System fÃ¼r zusÃ¤tzliche Features

---

### Ist JARVIS wirklich 100% lokal?

**Ja!** Nach der Installation benÃ¶tigt JARVIS **keine Internetverbindung** mehr.

**Was lokal lÃ¤uft:**
- âœ… AI-Modelle (GGUF Dateien auf deiner Festplatte)
- âœ… Alle Chats und Konversationen
- âœ… Plugins (auÃŸer Weather-Plugin braucht API)
- âœ… Frontend & Backend Server

**Was Internet braucht:**
- ğŸŒ **Initiale Installation** - Python packages, npm modules
- ğŸŒ **Model-Downloads** - GGUF Dateien von HuggingFace
- ğŸŒ **Plugin-APIs** - Weather Plugin braucht OpenWeatherMap

**Nach Setup:** Du kannst JARVIS **komplett offline** nutzen!

---

### Kostet JARVIS etwas?

**Nein! JARVIS ist 100% kostenlos.**

- âœ… **Open-Source** - Apache 2.0 Lizenz
- âœ… **Keine Abos** - Kein monatlicher/jÃ¤hrlicher Preis
- âœ… **Keine API-Kosten** - Kein OpenAI/Anthropic API nÃ¶tig
- âœ… **Keine versteckten Kosten** - Alles gratis

**Optional kostenpflichtig:**
- ğŸ’µ **Weather Plugin** - OpenWeatherMap API (~0â‚¬ fÃ¼r Free Tier)
- ğŸ’µ **Stromkosten** - GPU-Nutzung erhÃ¶ht Energieverbrauch minimal

---

### Welche Betriebssysteme werden unterstÃ¼tzt?

**Offizielle UnterstÃ¼tzung:**
- âœ… **Windows 10/11** - VollstÃ¤ndig getestet
- âœ… **Linux** - Ubuntu 20.04+, Debian, Fedora, Arch
- âœ… **macOS** - macOS 11+ (Big Sur und neuer)

**Voraussetzungen:**
- Python 3.11+
- Node.js 18+
- Git

---

## Installation & Setup

### Wie installiere ich JARVIS?

**Quick Start (empfohlen):**

```bash
# 1. Repository klonen
git clone https://github.com/Lautloserspieler/JarvisCore.git
cd JarvisCore

# 2. Dependencies installieren
pip install -r requirements.txt

# 3. llama.cpp Setup (automatisch!)
cd backend && python setup_llama.py && cd ..

# 4. Frontend Setup
cd frontend && npm install && cd ..

# 5. JARVIS starten
python main.py
```

**AusfÃ¼hrliche Anleitung:** [README.md](./README.md#installation--start)

---

### Brauche ich eine GPU?

**Nein, aber empfohlen!**

**Ohne GPU (CPU only):**
- âœ… Funktioniert einwandfrei
- âš¡ 5-10 tokens/Sekunde
- ğŸ’» Nutze kleine Modelle (Llama 3.2 3B, Phi-3 Mini)
- ğŸŒ Langsam bei groÃŸen Modellen (7B+)

**Mit NVIDIA GPU (CUDA):**
- âœ… Deutlich schneller
- âš¡âš¡âš¡ 30-50 tokens/Sekunde
- ğŸš€ GroÃŸe Modelle (7B-13B) laufen flÃ¼ssig
- ğŸ® Gaming-GPUs (RTX 3060+) perfekt

**Empfehlung:** CPU reicht fÃ¼r Chat, GPU fÃ¼r Power-User!

---

### Welche GPU wird unterstÃ¼tzt?

| GPU-Typ | Support | Setup | Performance |
|---------|---------|-------|-------------|
| **NVIDIA (CUDA)** | âœ… Voll | Automatisch | âš¡âš¡âš¡ 30-50 tok/s |
| **AMD (ROCm)** | âš ï¸ Experimentell | Komplex | âš¡âš¡ 25-40 tok/s |
| **Intel Arc** | ğŸ”„ Geplant | v1.2.0 | âš¡âš¡ 20-35 tok/s |
| **Apple Silicon (Metal)** | ğŸ”„ Geplant | v1.3.0 | âš¡âš¡ 25-40 tok/s |
| **CPU (x64 AVX2)** | âœ… Voll | Automatisch | âš¡ 5-10 tok/s |

**AMD GPU Nutzer:** ROCm Installation ist sehr komplex - **nutze CPU-Version!**

**NVIDIA Empfehlung:**
- Minimum: GTX 1660 (6GB VRAM)
- Empfohlen: RTX 3060 (12GB VRAM)
- Optimal: RTX 4070+ (12GB+ VRAM)

---

### Wie viel RAM brauche ich?

**Minimum: 8 GB**
- âœ… Kleine Modelle (3B) - 4-6 GB RAM
- âš ï¸ Mittlere Modelle (7B) - knapp, kann swappen
- âŒ GroÃŸe Modelle (13B+) - nicht nutzbar

**Empfohlen: 16 GB**
- âœ… Kleine Modelle (3B) - perfekt
- âœ… Mittlere Modelle (7B) - gut
- âœ… GroÃŸe Modelle (13B) - mÃ¶glich

**Optimal: 32 GB+**
- âœ… Alle Modelle problemlos
- âœ… Mehrere Modelle gleichzeitig
- âœ… GroÃŸer Chat-Kontext (32K tokens)

---

### Wie viel Speicherplatz wird benÃ¶tigt?

**Basis-Installation: ~2 GB**
- Python packages: ~500 MB
- Node modules: ~800 MB
- JARVIS Code: ~200 MB
- Logs/Config: ~50 MB

**Pro Modell: 2-15 GB**
- Llama 3.2 3B: ~2.0 GB
- Qwen 2.5 7B: ~5.2 GB
- Mistral 7B: ~7.5 GB
- Llama 3.1 70B: ~40 GB (falls geplant)

**Empfohlener freier Speicher: 20 GB+**

---

## Modelle

### Welche AI-Modelle kann ich nutzen?

**Pre-configured (7 Modelle):**

1. **Llama 3.2 3B** - Klein, schnell, Chat
2. **Phi-3 Mini** - Kompakt, effizient
3. **Qwen 2.5 7B** - Vielseitig, multilingual
4. **Mistral 7B Nemo** - Code, technisch
5. **DeepSeek Coder 6.7B** - Programmierung
6. **DeepSeek R1 8B** - Advanced Reasoning
7. **Llama 3.1 8B** - General Purpose

**Custom Models:**
- âœ… Alle GGUF-kompatiblen Modelle von HuggingFace
- âœ… Eigene Fine-Tunes (GGUF Format)
- âœ… Ollama Model Library (via Import)

Siehe: [Model List](./docs/MODEL_LIST.md)

---

### Wie downloade ich ein Modell?

**Via UI (empfohlen):**

1. JARVIS starten: `python main.py`
2. Browser: http://localhost:5050
3. **Models Tab** Ã¶ffnen
4. Modell wÃ¤hlen â†’ **Download** klicken
5. Quantization wÃ¤hlen (Q4_K_M empfohlen)
6. Warten (2-15 GB Download)
7. **Load** klicken zum Aktivieren

**Via CLI (fortgeschritten):**

```bash
python core/model_downloader.py --model llama-3.2-3b --quantization Q4_K_M
```

---

### Welches Modell ist am schnellsten?

**FÃ¼r CPU:**
1. ğŸ¥‡ **Llama 3.2 3B** - 8-12 tok/s
2. ğŸ¥ˆ **Phi-3 Mini** - 7-10 tok/s
3. ğŸ¥‰ **Qwen 2.5 7B** - 5-8 tok/s

**FÃ¼r GPU (NVIDIA):**
1. ğŸ¥‡ **Llama 3.2 3B** - 50-80 tok/s
2. ğŸ¥ˆ **Phi-3 Mini** - 45-70 tok/s
3. ğŸ¥‰ **Qwen 2.5 7B** - 35-50 tok/s

**Empfehlung:** Starte mit **Llama 3.2 3B** - beste Balance!

---

### Kann ich eigene Modelle nutzen?

**Ja!** Jedes GGUF-Format Modell funktioniert.

**So geht's:**

1. GGUF-Datei herunterladen (HuggingFace)
2. In `models/llm/` kopieren
3. In UI: Models Tab â†’ "Add Custom Model"
4. Pfad auswÃ¤hlen â†’ Laden

**Konvertierung (falls nÃ¶tig):**
```bash
# PyTorch â†’ GGUF
python llama.cpp/convert.py /path/to/model
```

Siehe: [Custom Models Guide](./docs/CUSTOM_MODELS.md)

---

### Wie groÃŸ sind die Modelle?

| Modell | Unquantized | Q4_K_M | Q5_K_M | Q8_0 |
|--------|-------------|--------|--------|------|
| **3B** | ~6 GB | ~2.0 GB | ~2.5 GB | ~3.5 GB |
| **7B** | ~14 GB | ~4.5 GB | ~5.5 GB | ~7.5 GB |
| **13B** | ~26 GB | ~8 GB | ~10 GB | ~14 GB |
| **70B** | ~140 GB | ~40 GB | ~50 GB | ~75 GB |

**Empfehlung:** Q4_K_M = beste Kompression bei guter QualitÃ¤t

---

## Performance

### Wie schnell ist JARVIS?

**Benchmark (Tokens pro Sekunde):**

| Hardware | Llama 3B | Qwen 7B | Mistral 7B |
|----------|----------|---------|------------|
| **RTX 4090** | 80-100 | 50-70 | 45-65 |
| **RTX 4070** | 60-80 | 40-55 | 35-50 |
| **RTX 3060** | 40-60 | 30-45 | 25-40 |
| **AMD Ryzen 9** | 10-15 | 6-10 | 5-8 |
| **Intel i7** | 8-12 | 5-8 | 4-7 |

**Zum Vergleich:**
- ChatGPT (Cloud): ~20-40 tok/s
- Claude (Cloud): ~25-45 tok/s
- JARVIS (RTX 3060): ~30-45 tok/s âœ…

---

### Warum ist mein JARVIS langsam?

**MÃ¶gliche GrÃ¼nde:**

1. **Zu groÃŸes Modell**
   - ğŸ”§ LÃ¶sung: Nutze kleineres Modell (3B statt 7B)

2. **CPU statt GPU**
   - ğŸ”§ LÃ¶sung: GPU aktivieren (CUDA setup)

3. **Zu wenig RAM**
   - ğŸ”§ LÃ¶sung: Kleineres Modell oder RAM upgrade

4. **Langer Chat-Kontext**
   - ğŸ”§ LÃ¶sung: Chat lÃ¶schen oder Context Window reduzieren

5. **Hintergrund-Apps**
   - ğŸ”§ LÃ¶sung: Andere GPU-Apps schlieÃŸen

---

### Kann ich die Performance verbessern?

**Ja! Mehrere MÃ¶glichkeiten:**

**1. GPU nutzen (grÃ¶ÃŸter Impact!)**
```bash
cd backend
python setup_llama.py  # WÃ¤hle CUDA
```

**2. Kleineres Modell wÃ¤hlen**
- Llama 3.2 3B statt Qwen 7B
- Q4 statt Q8 Quantization

**3. Context Window reduzieren**
- Settings â†’ Max Context: 2048 statt 8192

**4. Batch Size erhÃ¶hen**
- Settings â†’ Batch Size: 512 (GPU) / 128 (CPU)

**5. Thread Count optimieren**
- CPU: Nutze Kernel/2 (z.B. 8 Kerne = 4 Threads)

Siehe: [Performance Guide](./docs/PERFORMANCE.md)

---

### AMD GPU - Warum so langsam?

**ROCm ist komplex und instabil.**

**Probleme:**
- âŒ Komplizierte Installation (~2-3h)
- âŒ HÃ¤ufige Fehler und Crashes
- âŒ Schlechte Treiber-UnterstÃ¼tzung
- âŒ Nur bestimmte AMD GPUs unterstÃ¼tzt
- âŒ Windows ROCm = experimental

**Empfehlung: Nutze CPU-Version!**

CPU (5-10 tok/s) ist **stabiler** als ROCm mit Problemen.

**Zukunft:** Intel Arc Support (v1.2.0) wird besser sein.

---

## Features & Nutzung

### Kann JARVIS Bilder generieren?

**Aktuell: Nein.**

**Geplant fÃ¼r v2.0+ (Q2 2026):**
- Stable Diffusion Integration
- Lokale Image Generation
- Text-to-Image & Image-to-Image

**Workaround:** Nutze externes Tool (Automatic1111, ComfyUI)

---

### Funktioniert Spracherkennung?

**Teilweise.**

**Was funktioniert:**
- âœ… Voice Input (Web Speech API)
- âœ… Browser-basiert (Chrome, Edge)
- âœ… Visualisierung in UI

**Was NICHT funktioniert:**
- âŒ Voice Output (TTS) - geplant v1.2.0
- âŒ Offline Voice Input - geplant v1.2.0 (Whisper)

**Roadmap:**
- v1.2.0: Whisper (Speech-to-Text) + XTTS (Text-to-Speech)

---

### Kann JARVIS im Internet suchen?

**Aktuell: Nein.**

**Geplant:**
- v1.2.0: Web Search Plugin (Google/DuckDuckGo API)
- v2.0.0: Integrated Web Browsing

**Workaround:** Kopiere Infos manuell in Chat

---

### Wie aktiviere ich Plugins?

**So geht's:**

1. JARVIS starten
2. **Plugins Tab** Ã¶ffnen
3. Plugin auswÃ¤hlen (z.B. Weather)
4. **"Aktivieren"** klicken
5. Falls API-Key nÃ¶tig â†’ Modal Ã¶ffnet sich
6. API-Key eingeben â†’ Speichern
7. Plugin ist aktiv! âœ…

**VerfÃ¼gbare Plugins:**
- â˜€ï¸ Weather (braucht OpenWeatherMap API)
- â° Timer
- ğŸ“ Notes
- ğŸ“° News

**Eigene Plugins:** Siehe [Plugin Development](./docs/PLUGIN_DEV.md)

---

### Kann ich meinen Chat-Verlauf speichern?

**Ja! Automatisch.**

**Wo gespeichert:**
- `data/conversations/` - Alle Chats als JSON
- `data/memory/` - Kontext und Erinnerungen

**Export:**
- Settings â†’ Export Chat (als JSON/TXT)

**LÃ¶schen:**
- Chat Tab â†’ "Clear History"
- Oder manuell: `data/conversations/` lÃ¶schen

---

## Troubleshooting

### JARVIS startet nicht - was tun?

**Checklist:**

1. **Python Version prÃ¼fen:**
```bash
python --version  # Muss 3.11+ sein
```

2. **Dependencies installiert?**
```bash
pip install -r requirements.txt
cd frontend && npm install
```

3. **Ports frei?**
```bash
# Windows
netstat -ano | findstr :5050
netstat -ano | findstr :5050

# Linux/Mac
lsof -i :5050
lsof -i :5050
```

4. **Logs checken:**
```bash
cat logs/backend.log
cat logs/frontend.log
```

5. **Neustart:**
```bash
python main.py
```

---

### Port bereits belegt - wie beheben?

**LÃ¶sung 1: Prozess beenden**

```bash
# Windows
netstat -ano | findstr :5050
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :5050
kill -9 <PID>
```

**LÃ¶sung 2: Port Ã¤ndern**

Bearbeite `backend/.env`:
```env
BACKEND_PORT=5051  # Statt 5050
FRONTEND_PORT=5001  # Statt 5000
```

---

### Module not found Error

**Error:**
```
ModuleNotFoundError: No module named 'fastapi'
```

**LÃ¶sung:**
```bash
# Backend Dependencies
cd backend
pip install -r requirements.txt

# Frontend Dependencies  
cd frontend
npm install

# Core Dependencies
cd ..
pip install -r requirements.txt
```

---

### GPU wird nicht erkannt

**Checklist:**

1. **NVIDIA GPU vorhanden?**
```bash
nvidia-smi  # Zeigt GPU Info
```

2. **CUDA installiert?**
```bash
nvcc --version  # CUDA Compiler
```

3. **llama.cpp mit CUDA?**
```bash
cd backend
python setup_llama.py  # Neu installieren
```

4. **Treiber aktuell?**
- NVIDIA: GeForce Experience updaten
- Minimum: CUDA 11.8 Treiber

---

### Frontend lÃ¤dt nicht

**MÃ¶gliche Ursachen:**

**1. Backend nicht gestartet:**
```bash
# Backend manuell starten
cd backend
python main.py
```

**2. Port-Konflikt:**
- Siehe: [Port bereits belegt](#port-bereits-belegt---wie-beheben)

**3. Browser-Cache:**
- Strg+F5 (Hard Refresh)
- Oder: Inkognito-Modus testen

**4. npm Build Fehler:**
```bash
cd frontend
npm run build
```

---

## Sicherheit & PrivatsphÃ¤re

### Werden meine Daten gesammelt?

**Nein! Absolut nicht.**

**JARVIS sammelt:**
- âŒ Keine Telemetrie
- âŒ Keine Analytics
- âŒ Keine Crash Reports
- âŒ Keine Chat-Logs an Server
- âŒ Keine Nutzungsstatistiken

**Alles bleibt lokal:**
- âœ… Chats: `data/conversations/`
- âœ… Config: `config/settings.json`
- âœ… Logs: `logs/`

**100% Privacy by Design** ğŸ”’

---

### Ist JARVIS sicher?

**Ja! Mehrere Sicherheitsebenen:**

**Code-Level:**
- âœ… **Open-Source** - Code ist einsehbar
- âœ… **Keine Third-Party Tracking**
- âœ… **Input Validation** - Schutz vor Injection
- âœ… **API Key Encryption** - Sichere Speicherung

**System-Level:**
- âœ… **Local-Only** - Keine Cloud-Verbindung
- âœ… **Sandboxed Plugins** - Isolierte AusfÃ¼hrung
- âœ… **No Sudo Required** - LÃ¤uft als User

**Updates:**
- âœ… **Dependabot** - Automatische Security Updates
- âœ… **CI/CD Scans** - Code Quality Checks

Siehe: [SECURITY.md](./SECURITY.md)

---

### Wo werden Chat-Daten gespeichert?

**Lokale Ordner:**

```
JarvisCore/
â””â”€â”€ data/
    â”œâ”€â”€ conversations/     # Alle Chats (JSON)
    â”œâ”€â”€ memory/             # Kontext & Erinnerungen
    â””â”€â”€ user_data/          # Notizen, Einstellungen
```

**Format:** JSON (plain text, nicht verschlÃ¼sselt)

**LÃ¶schen:**
```bash
rm -rf data/conversations/*  # Alle Chats lÃ¶schen
```

**Backup:**
```bash
cp -r data/ backup_$(date +%Y%m%d)/
```

---

### Kann ich Telemetrie deaktivieren?

**Nicht nÃ¶tig - es gibt keine!**

JARVIS hat **keinerlei Telemetrie** eingebaut.

**Proof:** Suche im Code nach:
```bash
grep -r "analytics" .
grep -r "telemetry" .
grep -r "tracking" .
# = Keine Treffer auÃŸer Kommentare
```

**Network Activity:**
- Nach Installation: 0 Requests an externe Server
- Nur lokal: localhost:5050

---

## Entwicklung & Community

### Kann ich zu JARVIS beitragen?

**Ja! Contributions sind willkommen! ğŸ¤**

**Wie:**

1. **Fork** das Repository
2. **Branch** erstellen: `feature/meine-idee`
3. **Coden** + Tests schreiben
4. **Commit**: `git commit -m 'feat: Meine Idee'`
5. **Push**: `git push origin feature/meine-idee`
6. **Pull Request** erstellen

**Was wird gebraucht:**
- ğŸ› Bug Fixes
- âœ¨ Neue Features
- ğŸ“š Dokumentation
- ğŸŒ Ãœbersetzungen
- ğŸ”Œ Plugins
- ğŸ¨ UI/UX Improvements

Siehe: [CONTRIBUTING.md](./CONTRIBUTING.md)

---

### Wie melde ich Bugs?

**GitHub Issues:**

1. Gehe zu: [Issues](https://github.com/Lautloserspieler/JarvisCore/issues)
2. Klick "New Issue"
3. WÃ¤hle Template: "Bug Report"
4. FÃ¼lle aus:
   - Was passiert?
   - Was sollte passieren?
   - Schritte zum Reproduzieren
   - Logs (falls vorhanden)
   - System Info (OS, Python, GPU)

**Oder:** Direkter Link zum [Bug Report](https://github.com/Lautloserspieler/JarvisCore/issues/new?template=bug_report.md)

---

### Gibt es einen Discord/Community?

**Geplant fÃ¼r nach Launch!**

**Aktuell:**
- ğŸ› **GitHub Issues** - Bugs & Feature Requests
- ğŸ’¬ **GitHub Discussions** - Community Q&A (kommt bald)

**Bald:**
- ğŸ’¬ **Discord Server** - Community Chat (Q1 2026)
- ğŸ¦ **Twitter/X** - Updates & News
- ğŸ“§ **Newsletter** - Release Notes

Stay tuned! ğŸš€

---

### Roadmap - Was kommt als nÃ¤chstes?

**v1.2.0 (Q1 2026):**
- ğŸ¤ Voice Input (Whisper)
- ğŸ”Š Voice Output (XTTS v2)
- ğŸ–¥ï¸ Desktop App (Wails)
- ğŸ³ Docker Support
- ğŸ”Œ Mehr Plugins

**v2.0.0 (Q2 2026):**
- ğŸ“š RAG System (Dokument-Suche)
- ğŸ—ƒï¸ Vector Database
- ğŸ‘¥ Multi-User Support
- â˜ï¸ Cloud Deployment Option

**v3.0+ (2027+):**
- ğŸ¤– Multi-Agent System
- ğŸ“¸ Image Generation (Stable Diffusion)
- ğŸ” Web Browsing
- ğŸ¬ Video Analysis

Siehe: [CHANGELOG.md](./CHANGELOG.md#unreleased---future-plans)

---

## ğŸ’¬ Weitere Hilfe

**Docs:**
- [ğŸ“š README](./README.md)
- [ğŸ“– Documentation](./docs/)
- [ğŸ”’ Security](./SECURITY.md)
- [ğŸ¤ Contributing](./CONTRIBUTING.md)

**Support:**
- [ğŸ› GitHub Issues](https://github.com/Lautloserspieler/JarvisCore/issues)
- [ğŸ’¬ Discussions](https://github.com/Lautloserspieler/JarvisCore/discussions) (coming soon)

**Contact:**
- ğŸ“§ Email: security@jarviscore.de (Security only)
- ğŸ¦ Twitter: @JarvisCore (coming soon)

---

<div align="center">

**Made with â¤ï¸ by Lautloserspieler**

*"Sometimes you gotta run before you can walk."* - Tony Stark

[â­ Star on GitHub](https://github.com/Lautloserspieler/JarvisCore) | [ğŸ› Report Bug](https://github.com/Lautloserspieler/JarvisCore/issues) | [ğŸ’¡ Request Feature](https://github.com/Lautloserspieler/JarvisCore/issues)

</div>
