{
  "run": [
    {
      "method": "shell.run",
      "params": {
        "message": "{{platform === 'win32' ? '(if exist app rmdir /s /q app) && git clone https://github.com/Lautloserspieler/JarvisCore app' : 'rm -rf app && git clone https://github.com/Lautloserspieler/JarvisCore app'}}"
      }
    },
    {
      "method": "input",
      "params": {
        "title": "GPU Acceleration",
        "description": "WÃ¤hle deine GPU-Beschleunigung fÃ¼r llama.cpp:",
        "type": "select",
        "options": [
          {
            "value": "cuda",
            "text": "ðŸŸ¢ NVIDIA GPU (CUDA) - Empfohlen fÃ¼r NVIDIA RTX/GTX Karten"
          },
          {
            "value": "rocm",
            "text": "ðŸŸ  AMD GPU (ROCm) - FÃ¼r AMD Radeon RX 5000+/7000+ (Experimentell)"
          },
          {
            "value": "cpu",
            "text": "ðŸ”µ CPU Only - Keine GPU (Funktioniert auf allen Computern)"
          }
        ]
      }
    },
    {
      "method": "local.set",
      "params": {
        "gpu_type": "{{input}}"
      }
    },
    {
      "method": "shell.run",
      "params": {
        "path": "app",
        "venv": "venv",
        "message": "pip install -e \".[tts]\""
      }
    },
    {
      "method": "shell.run",
      "params": {
        "path": "app/frontend",
        "message": "npm install"
      }
    },
    {
      "method": "script",
      "params": {
        "run": [{
          "method": "{{local.gpu_type === 'cuda' ? 'shell.run' : 'process.return'}}",
          "params": {
            "path": "app",
            "venv": "venv",
            "message": "pip uninstall llama-cpp-python -y && set CMAKE_ARGS=-DLLAMA_CUDA=on && pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python"
          }
        }]
      }
    },
    {
      "method": "script",
      "params": {
        "run": [{
          "method": "{{local.gpu_type === 'rocm' ? 'shell.run' : 'process.return'}}",
          "params": {
            "path": "app",
            "venv": "venv",
            "message": "pip uninstall llama-cpp-python -y && set CMAKE_ARGS=-DLLAMA_HIPBLAS=on && pip install llama-cpp-python --force-reinstall --no-cache-dir --no-binary llama-cpp-python"
          }
        }]
      }
    },
    {
      "method": "notify",
      "params": {
        "html": "Installation complete!<br><br><b>GPU Type:</b> {{local.gpu_type === 'cuda' ? 'NVIDIA CUDA ðŸŸ¢' : local.gpu_type === 'rocm' ? 'AMD ROCm ðŸŸ ' : 'CPU Only ðŸ”µ'}}<br><br>Click 'Start' to launch JARVIS."
      }
    }
  ]
}